{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T17:50:10.592261Z",
     "start_time": "2025-03-25T17:49:58.903759Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:50:19.292134Z",
     "start_time": "2025-03-25T14:50:19.260275Z"
    }
   },
   "source": [
    "# Create directories for saving results\n",
    "os.makedirs('generated_samples', exist_ok=True)\n",
    "os.makedirs('model_checkpoints', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# num_of_classes = 10  # 10 types mentioned in the paper\n",
    "num_of_classes = 8  # 8 weather types from spatial synoptic classification"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T17:50:14.551747Z",
     "start_time": "2025-03-25T17:50:14.533747Z"
    }
   },
   "source": [
    "class WeatherClassificationWGANGP:\n",
    "    def __init__(self, img_shape=(64, 64, 3), num_classes=10, latent_dim=100, gradient_penalty_weight=10):\n",
    "        self.img_shape = img_shape\n",
    "        self.num_classes = num_classes  # 10 types of weather status as mentioned in the paper\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gradient_penalty_weight = gradient_penalty_weight\n",
    "        \n",
    "        # Following WGAN-GP paper, we use Adam optimizer\n",
    "        self.generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0, beta_2=0.9)\n",
    "        self.discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0, beta_2=0.9)\n",
    "        \n",
    "        # Build models\n",
    "        self.generator = self.build_generator()\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        \n",
    "    def build_generator(self):\n",
    "        \"\"\"Build the generator model as described in the paper\"\"\"\n",
    "        model = models.Sequential()\n",
    "        \n",
    "        # Foundation for 4x4 feature maps\n",
    "        model.add(layers.Dense(4 * 4 * 256, use_bias=False, input_shape=(self.latent_dim,)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        model.add(layers.Reshape((4, 4, 256)))\n",
    "        \n",
    "        # Upsampling layers\n",
    "        model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(layers.Conv2DTranspose(self.img_shape[2], (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "        \"\"\"Build the discriminator model as described in the paper\"\"\"\n",
    "        model = models.Sequential()\n",
    "        \n",
    "        # No batch normalization in critic as per WGAN-GP paper\n",
    "        model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=self.img_shape))\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        model.add(layers.Dropout(0.3))\n",
    "        \n",
    "        model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        model.add(layers.Dropout(0.3))\n",
    "        \n",
    "        model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        model.add(layers.Dropout(0.3))\n",
    "        \n",
    "        model.add(layers.Flatten())\n",
    "        \n",
    "        # Weather classification output\n",
    "        model.add(layers.Dense(self.num_classes))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def gradient_penalty(self, real_images, fake_images):\n",
    "        \"\"\"Calculate the gradient penalty for WGAN-GP\"\"\"\n",
    "        batch_size = real_images.shape[0]\n",
    "        \n",
    "        # Create random interpolation points between real and fake images\n",
    "        alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        interpolated = alpha * real_images + (1 - alpha) * fake_images\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(interpolated)\n",
    "            # Get critic output for interpolated images\n",
    "            interpolated_output = self.discriminator(interpolated, training=True)\n",
    "        \n",
    "        # Calculate gradients with respect to inputs\n",
    "        gradients = tape.gradient(interpolated_output, interpolated)\n",
    "        # Calculate norm of gradients\n",
    "        gradients_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n",
    "        # Calculate gradient penalty\n",
    "        gradient_penalty = tf.reduce_mean((gradients_norm - 1.0) ** 2)\n",
    "        \n",
    "        return gradient_penalty\n",
    "    \n",
    "    @tf.function\n",
    "    def train_discriminator(self, real_images, labels):\n",
    "        \"\"\"Train the discriminator (critic) following WGAN-GP approach\"\"\"\n",
    "        batch_size = real_images.shape[0]\n",
    "        noise = tf.random.normal([batch_size, self.latent_dim])\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images\n",
    "            fake_images = self.generator(noise, training=True)\n",
    "            \n",
    "            # Get critic output for real and fake images\n",
    "            real_output = self.discriminator(real_images, training=True)\n",
    "            fake_output = self.discriminator(fake_images, training=True)\n",
    "            \n",
    "            # Calculate Wasserstein loss\n",
    "            real_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=labels, logits=real_output))\n",
    "            fake_loss = tf.reduce_mean(tf.reduce_mean(fake_output, axis=1))\n",
    "            \n",
    "            # Calculate gradient penalty\n",
    "            gp = self.gradient_penalty(real_images, fake_images)\n",
    "            \n",
    "            # Total discriminator loss\n",
    "            disc_loss = real_loss + fake_loss + self.gradient_penalty_weight * gp\n",
    "        \n",
    "        # Get gradients and update weights\n",
    "        gradients = tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "        self.discriminator_optimizer.apply_gradients(\n",
    "            zip(gradients, self.discriminator.trainable_variables))\n",
    "        \n",
    "        return disc_loss, real_loss, fake_loss, gp\n",
    "    \n",
    "    @tf.function\n",
    "    def train_generator(self):\n",
    "        \"\"\"Train the generator following WGAN-GP approach\"\"\"\n",
    "        batch_size = 64  # Fixed batch size for generator\n",
    "        noise = tf.random.normal([batch_size, self.latent_dim])\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images\n",
    "            fake_images = self.generator(noise, training=True)\n",
    "            \n",
    "            # Get critic output for fake images\n",
    "            fake_output = self.discriminator(fake_images, training=True)\n",
    "            \n",
    "            # Calculate generator loss\n",
    "            gen_loss = -tf.reduce_mean(tf.reduce_mean(fake_output, axis=1))\n",
    "        \n",
    "        # Get gradients and update weights\n",
    "        gradients = tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        self.generator_optimizer.apply_gradients(\n",
    "            zip(gradients, self.generator.trainable_variables))\n",
    "        \n",
    "        return gen_loss\n",
    "    \n",
    "    def train(self, dataset, epochs, batch_size=64, n_critic=5):\n",
    "        \"\"\"Train the WGAN-GP model\"\"\"\n",
    "        steps_per_epoch = len(dataset) // batch_size\n",
    "        \n",
    "        # Lists to store loss values for plotting\n",
    "        disc_losses = []\n",
    "        gen_losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_disc_losses = []\n",
    "            epoch_gen_losses = []\n",
    "            \n",
    "            for step in range(steps_per_epoch):\n",
    "                # Get real images batch\n",
    "                indices = np.random.randint(0, len(dataset), batch_size)\n",
    "                real_images = dataset[indices]\n",
    "                labels = np.random.randint(0, self.num_classes, batch_size)  # Random labels for demonstration\n",
    "                \n",
    "                # Train discriminator for n_critic iterations\n",
    "                for _ in range(n_critic):\n",
    "                    d_loss, real_loss, fake_loss, gp = self.train_discriminator(real_images, labels)\n",
    "                    epoch_disc_losses.append(d_loss.numpy())\n",
    "                \n",
    "                # Train generator\n",
    "                g_loss = self.train_generator()\n",
    "                epoch_gen_losses.append(g_loss.numpy())\n",
    "                \n",
    "            # Append average losses for this epoch\n",
    "            disc_losses.append(np.mean(epoch_disc_losses))\n",
    "            gen_losses.append(np.mean(epoch_gen_losses))\n",
    "            \n",
    "            # Print progress and generate samples\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "                print(f\"D loss: {disc_losses[-1]:.4f}, G loss: {gen_losses[-1]:.4f}\")\n",
    "                self.generate_and_save_images(epoch + 1)\n",
    "                \n",
    "                # Plot loss curves\n",
    "                self.plot_losses(disc_losses, gen_losses, epoch + 1)\n",
    "            \n",
    "            # Save model checkpoints\n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                self.generator.save(f'model_checkpoints/generator_epoch_{epoch+1}.h5')\n",
    "                self.discriminator.save(f'model_checkpoints/discriminator_epoch_{epoch+1}.h5')\n",
    "    \n",
    "    def generate_and_save_images(self, epoch):\n",
    "        \"\"\"Generate and save sample images\"\"\"\n",
    "        noise = tf.random.normal([16, self.latent_dim])\n",
    "        generated_images = self.generator(noise, training=False)\n",
    "        generated_images = (generated_images + 1) / 2.0  # Rescale to [0, 1]\n",
    "        \n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        for i in range(generated_images.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            plt.imshow(generated_images[i, :, :, :])\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.savefig(f'generated_samples/epoch_{epoch}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_losses(self, disc_losses, gen_losses, epoch):\n",
    "        \"\"\"Plot and save loss curves\"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(disc_losses, label='Discriminator Loss')\n",
    "        plt.plot(gen_losses, label='Generator Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title(f'WGAN-GP Training Losses (Epoch {epoch})')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/losses_epoch_{epoch}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def generate_synthetic_data(self, n_samples, labels=None):\n",
    "        \"\"\"Generate synthetic weather images\"\"\"\n",
    "        noise = tf.random.normal([n_samples, self.latent_dim])\n",
    "        generated_images = self.generator(noise, training=False)\n",
    "        \n",
    "        # If labels are not provided, generate random labels\n",
    "        if labels is None:\n",
    "            labels = np.random.randint(0, self.num_classes, n_samples)\n",
    "        \n",
    "        return generated_images, labels\n",
    "    \n",
    "    def classify_weather(self, images):\n",
    "        \"\"\"Classify weather images using the trained discriminator\"\"\"\n",
    "        logits = self.discriminator(images, training=False)\n",
    "        return tf.nn.softmax(logits)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T15:05:01.836853Z",
     "start_time": "2025-03-25T14:50:30.025679Z"
    }
   },
   "source": [
    "import requests\n",
    "import ftplib\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def download_surfrad_data(station='dra', start_date='2014-01-01', end_date='2015-12-31'):\n",
    "    \"\"\"\n",
    "    Download SURFRAD data for a specific station and date range\n",
    "    station: 'dra' for Desert Rock\n",
    "    \"\"\"\n",
    "    # Base URL for SURFRAD data\n",
    "    base_url = \"https://gml.noaa.gov/aftp/data/radiation/surfrad/\"\n",
    "    \n",
    "    # Parse dates\n",
    "    start = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    \n",
    "    # Create directory for data\n",
    "    os.makedirs(f'surfrad_data/{station}', exist_ok=True)\n",
    "    \n",
    "    # Loop through all dates\n",
    "    current_date = start\n",
    "    while current_date <= end:\n",
    "        year = current_date.year\n",
    "        month = current_date.month\n",
    "        day = current_date.day\n",
    "\n",
    "        for day in range(1, 365):\n",
    "            # Construct URL for specific date\n",
    "            year_formatted = str(year)[2:]\n",
    "            file_name = f\"{station}{year_formatted}{day:03d}.dat\"\n",
    "            url = f\"{base_url}{station}/{year}/{file_name}\"\n",
    "            \n",
    "            # Download file\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    with open(f'surfrad_data/{station}/{file_name}', 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "                    print(f\"Downloaded {file_name}\")\n",
    "                else:\n",
    "                    print(f\"Failed to download {file_name}: {response.status_code}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {file_name}: {e}\")\n",
    "        \n",
    "        # Move to next day\n",
    "        current_date += timedelta(days=365)\n",
    "\n",
    "# Example usage\n",
    "download_surfrad_data(station='dra', start_date='2014-01-01', end_date='2015-12-31')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded dra14001.dat\n",
      "Downloaded dra14002.dat\n",
      "Downloaded dra14003.dat\n",
      "Downloaded dra14004.dat\n",
      "Downloaded dra14005.dat\n",
      "Downloaded dra14006.dat\n",
      "Downloaded dra14007.dat\n",
      "Downloaded dra14008.dat\n",
      "Downloaded dra14009.dat\n",
      "Downloaded dra14010.dat\n",
      "Downloaded dra14011.dat\n",
      "Downloaded dra14012.dat\n",
      "Downloaded dra14013.dat\n",
      "Downloaded dra14014.dat\n",
      "Downloaded dra14015.dat\n",
      "Downloaded dra14016.dat\n",
      "Downloaded dra14017.dat\n",
      "Downloaded dra14018.dat\n",
      "Downloaded dra14019.dat\n",
      "Downloaded dra14020.dat\n",
      "Downloaded dra14021.dat\n",
      "Downloaded dra14022.dat\n",
      "Downloaded dra14023.dat\n",
      "Downloaded dra14024.dat\n",
      "Downloaded dra14025.dat\n",
      "Downloaded dra14026.dat\n",
      "Downloaded dra14027.dat\n",
      "Downloaded dra14028.dat\n",
      "Downloaded dra14029.dat\n",
      "Downloaded dra14030.dat\n",
      "Downloaded dra14031.dat\n",
      "Downloaded dra14032.dat\n",
      "Downloaded dra14033.dat\n",
      "Downloaded dra14034.dat\n",
      "Downloaded dra14035.dat\n",
      "Downloaded dra14036.dat\n",
      "Downloaded dra14037.dat\n",
      "Downloaded dra14038.dat\n",
      "Downloaded dra14039.dat\n",
      "Downloaded dra14040.dat\n",
      "Downloaded dra14041.dat\n",
      "Downloaded dra14042.dat\n",
      "Downloaded dra14043.dat\n",
      "Downloaded dra14044.dat\n",
      "Downloaded dra14045.dat\n",
      "Downloaded dra14046.dat\n",
      "Downloaded dra14047.dat\n",
      "Downloaded dra14048.dat\n",
      "Downloaded dra14049.dat\n",
      "Downloaded dra14050.dat\n",
      "Downloaded dra14051.dat\n",
      "Downloaded dra14052.dat\n",
      "Downloaded dra14053.dat\n",
      "Downloaded dra14054.dat\n",
      "Downloaded dra14055.dat\n",
      "Downloaded dra14056.dat\n",
      "Downloaded dra14057.dat\n",
      "Downloaded dra14058.dat\n",
      "Downloaded dra14059.dat\n",
      "Downloaded dra14060.dat\n",
      "Downloaded dra14061.dat\n",
      "Downloaded dra14062.dat\n",
      "Downloaded dra14063.dat\n",
      "Downloaded dra14064.dat\n",
      "Downloaded dra14065.dat\n",
      "Downloaded dra14066.dat\n",
      "Downloaded dra14067.dat\n",
      "Downloaded dra14068.dat\n",
      "Downloaded dra14069.dat\n",
      "Downloaded dra14070.dat\n",
      "Downloaded dra14071.dat\n",
      "Downloaded dra14072.dat\n",
      "Downloaded dra14073.dat\n",
      "Downloaded dra14074.dat\n",
      "Downloaded dra14075.dat\n",
      "Downloaded dra14076.dat\n",
      "Downloaded dra14077.dat\n",
      "Downloaded dra14078.dat\n",
      "Downloaded dra14079.dat\n",
      "Downloaded dra14080.dat\n",
      "Downloaded dra14081.dat\n",
      "Downloaded dra14082.dat\n",
      "Downloaded dra14083.dat\n",
      "Downloaded dra14084.dat\n",
      "Downloaded dra14085.dat\n",
      "Downloaded dra14086.dat\n",
      "Downloaded dra14087.dat\n",
      "Downloaded dra14088.dat\n",
      "Downloaded dra14089.dat\n",
      "Downloaded dra14090.dat\n",
      "Downloaded dra14091.dat\n",
      "Downloaded dra14092.dat\n",
      "Downloaded dra14093.dat\n",
      "Downloaded dra14094.dat\n",
      "Downloaded dra14095.dat\n",
      "Downloaded dra14096.dat\n",
      "Downloaded dra14097.dat\n",
      "Downloaded dra14098.dat\n",
      "Downloaded dra14099.dat\n",
      "Downloaded dra14100.dat\n",
      "Downloaded dra14101.dat\n",
      "Downloaded dra14102.dat\n",
      "Downloaded dra14103.dat\n",
      "Downloaded dra14104.dat\n",
      "Downloaded dra14105.dat\n",
      "Downloaded dra14106.dat\n",
      "Downloaded dra14107.dat\n",
      "Downloaded dra14108.dat\n",
      "Downloaded dra14109.dat\n",
      "Downloaded dra14110.dat\n",
      "Downloaded dra14111.dat\n",
      "Downloaded dra14112.dat\n",
      "Downloaded dra14113.dat\n",
      "Downloaded dra14114.dat\n",
      "Downloaded dra14115.dat\n",
      "Downloaded dra14116.dat\n",
      "Downloaded dra14117.dat\n",
      "Downloaded dra14118.dat\n",
      "Downloaded dra14119.dat\n",
      "Downloaded dra14120.dat\n",
      "Downloaded dra14121.dat\n",
      "Downloaded dra14122.dat\n",
      "Downloaded dra14123.dat\n",
      "Downloaded dra14124.dat\n",
      "Downloaded dra14125.dat\n",
      "Downloaded dra14126.dat\n",
      "Downloaded dra14127.dat\n",
      "Downloaded dra14128.dat\n",
      "Downloaded dra14129.dat\n",
      "Downloaded dra14130.dat\n",
      "Downloaded dra14131.dat\n",
      "Downloaded dra14132.dat\n",
      "Downloaded dra14133.dat\n",
      "Downloaded dra14134.dat\n",
      "Downloaded dra14135.dat\n",
      "Downloaded dra14136.dat\n",
      "Downloaded dra14137.dat\n",
      "Downloaded dra14138.dat\n",
      "Downloaded dra14139.dat\n",
      "Downloaded dra14140.dat\n",
      "Downloaded dra14141.dat\n",
      "Downloaded dra14142.dat\n",
      "Downloaded dra14143.dat\n",
      "Downloaded dra14144.dat\n",
      "Downloaded dra14145.dat\n",
      "Downloaded dra14146.dat\n",
      "Downloaded dra14147.dat\n",
      "Downloaded dra14148.dat\n",
      "Downloaded dra14149.dat\n",
      "Downloaded dra14150.dat\n",
      "Downloaded dra14151.dat\n",
      "Downloaded dra14152.dat\n",
      "Downloaded dra14153.dat\n",
      "Downloaded dra14154.dat\n",
      "Downloaded dra14155.dat\n",
      "Downloaded dra14156.dat\n",
      "Downloaded dra14157.dat\n",
      "Downloaded dra14158.dat\n",
      "Downloaded dra14159.dat\n",
      "Downloaded dra14160.dat\n",
      "Downloaded dra14161.dat\n",
      "Downloaded dra14162.dat\n",
      "Downloaded dra14163.dat\n",
      "Downloaded dra14164.dat\n",
      "Downloaded dra14165.dat\n",
      "Downloaded dra14166.dat\n",
      "Downloaded dra14167.dat\n",
      "Downloaded dra14168.dat\n",
      "Downloaded dra14169.dat\n",
      "Downloaded dra14170.dat\n",
      "Downloaded dra14171.dat\n",
      "Downloaded dra14172.dat\n",
      "Downloaded dra14173.dat\n",
      "Downloaded dra14174.dat\n",
      "Downloaded dra14175.dat\n",
      "Downloaded dra14176.dat\n",
      "Downloaded dra14177.dat\n",
      "Downloaded dra14178.dat\n",
      "Downloaded dra14179.dat\n",
      "Downloaded dra14180.dat\n",
      "Downloaded dra14181.dat\n",
      "Downloaded dra14182.dat\n",
      "Downloaded dra14183.dat\n",
      "Downloaded dra14184.dat\n",
      "Downloaded dra14185.dat\n",
      "Downloaded dra14186.dat\n",
      "Downloaded dra14187.dat\n",
      "Downloaded dra14188.dat\n",
      "Downloaded dra14189.dat\n",
      "Downloaded dra14190.dat\n",
      "Downloaded dra14191.dat\n",
      "Downloaded dra14192.dat\n",
      "Downloaded dra14193.dat\n",
      "Downloaded dra14194.dat\n",
      "Downloaded dra14195.dat\n",
      "Downloaded dra14196.dat\n",
      "Downloaded dra14197.dat\n",
      "Downloaded dra14198.dat\n",
      "Downloaded dra14199.dat\n",
      "Downloaded dra14200.dat\n",
      "Downloaded dra14201.dat\n",
      "Downloaded dra14202.dat\n",
      "Downloaded dra14203.dat\n",
      "Downloaded dra14204.dat\n",
      "Downloaded dra14205.dat\n",
      "Downloaded dra14206.dat\n",
      "Downloaded dra14207.dat\n",
      "Downloaded dra14208.dat\n",
      "Downloaded dra14209.dat\n",
      "Downloaded dra14210.dat\n",
      "Downloaded dra14211.dat\n",
      "Downloaded dra14212.dat\n",
      "Downloaded dra14213.dat\n",
      "Downloaded dra14214.dat\n",
      "Downloaded dra14215.dat\n",
      "Downloaded dra14216.dat\n",
      "Downloaded dra14217.dat\n",
      "Downloaded dra14218.dat\n",
      "Downloaded dra14219.dat\n",
      "Downloaded dra14220.dat\n",
      "Downloaded dra14221.dat\n",
      "Downloaded dra14222.dat\n",
      "Downloaded dra14223.dat\n",
      "Downloaded dra14224.dat\n",
      "Downloaded dra14225.dat\n",
      "Downloaded dra14226.dat\n",
      "Downloaded dra14227.dat\n",
      "Downloaded dra14228.dat\n",
      "Downloaded dra14229.dat\n",
      "Downloaded dra14230.dat\n",
      "Downloaded dra14231.dat\n",
      "Downloaded dra14232.dat\n",
      "Downloaded dra14233.dat\n",
      "Downloaded dra14234.dat\n",
      "Downloaded dra14235.dat\n",
      "Downloaded dra14236.dat\n",
      "Downloaded dra14237.dat\n",
      "Downloaded dra14238.dat\n",
      "Downloaded dra14239.dat\n",
      "Downloaded dra14240.dat\n",
      "Downloaded dra14241.dat\n",
      "Downloaded dra14242.dat\n",
      "Downloaded dra14243.dat\n",
      "Downloaded dra14244.dat\n",
      "Downloaded dra14245.dat\n",
      "Downloaded dra14246.dat\n",
      "Downloaded dra14247.dat\n",
      "Downloaded dra14248.dat\n",
      "Downloaded dra14249.dat\n",
      "Downloaded dra14250.dat\n",
      "Downloaded dra14251.dat\n",
      "Downloaded dra14252.dat\n",
      "Downloaded dra14253.dat\n",
      "Downloaded dra14254.dat\n",
      "Downloaded dra14255.dat\n",
      "Downloaded dra14256.dat\n",
      "Downloaded dra14257.dat\n",
      "Downloaded dra14258.dat\n",
      "Downloaded dra14259.dat\n",
      "Downloaded dra14260.dat\n",
      "Downloaded dra14261.dat\n",
      "Downloaded dra14262.dat\n",
      "Downloaded dra14263.dat\n",
      "Downloaded dra14264.dat\n",
      "Downloaded dra14265.dat\n",
      "Downloaded dra14266.dat\n",
      "Downloaded dra14267.dat\n",
      "Downloaded dra14268.dat\n",
      "Downloaded dra14269.dat\n",
      "Downloaded dra14270.dat\n",
      "Downloaded dra14271.dat\n",
      "Downloaded dra14272.dat\n",
      "Downloaded dra14273.dat\n",
      "Downloaded dra14274.dat\n",
      "Downloaded dra14275.dat\n",
      "Downloaded dra14276.dat\n",
      "Downloaded dra14277.dat\n",
      "Downloaded dra14278.dat\n",
      "Downloaded dra14279.dat\n",
      "Downloaded dra14280.dat\n",
      "Downloaded dra14281.dat\n",
      "Downloaded dra14282.dat\n",
      "Downloaded dra14283.dat\n",
      "Downloaded dra14284.dat\n",
      "Downloaded dra14285.dat\n",
      "Downloaded dra14286.dat\n",
      "Downloaded dra14287.dat\n",
      "Downloaded dra14288.dat\n",
      "Downloaded dra14289.dat\n",
      "Downloaded dra14290.dat\n",
      "Downloaded dra14291.dat\n",
      "Downloaded dra14292.dat\n",
      "Downloaded dra14293.dat\n",
      "Downloaded dra14294.dat\n",
      "Downloaded dra14295.dat\n",
      "Downloaded dra14296.dat\n",
      "Downloaded dra14297.dat\n",
      "Downloaded dra14298.dat\n",
      "Downloaded dra14299.dat\n",
      "Downloaded dra14300.dat\n",
      "Downloaded dra14301.dat\n",
      "Downloaded dra14302.dat\n",
      "Downloaded dra14303.dat\n",
      "Downloaded dra14304.dat\n",
      "Downloaded dra14305.dat\n",
      "Downloaded dra14306.dat\n",
      "Downloaded dra14307.dat\n",
      "Downloaded dra14308.dat\n",
      "Downloaded dra14309.dat\n",
      "Downloaded dra14310.dat\n",
      "Downloaded dra14311.dat\n",
      "Downloaded dra14312.dat\n",
      "Downloaded dra14313.dat\n",
      "Downloaded dra14314.dat\n",
      "Downloaded dra14315.dat\n",
      "Downloaded dra14316.dat\n",
      "Downloaded dra14317.dat\n",
      "Downloaded dra14318.dat\n",
      "Downloaded dra14319.dat\n",
      "Downloaded dra14320.dat\n",
      "Downloaded dra14321.dat\n",
      "Downloaded dra14322.dat\n",
      "Downloaded dra14323.dat\n",
      "Downloaded dra14324.dat\n",
      "Downloaded dra14325.dat\n",
      "Downloaded dra14326.dat\n",
      "Downloaded dra14327.dat\n",
      "Downloaded dra14328.dat\n",
      "Downloaded dra14329.dat\n",
      "Downloaded dra14330.dat\n",
      "Downloaded dra14331.dat\n",
      "Downloaded dra14332.dat\n",
      "Downloaded dra14333.dat\n",
      "Downloaded dra14334.dat\n",
      "Downloaded dra14335.dat\n",
      "Downloaded dra14336.dat\n",
      "Downloaded dra14337.dat\n",
      "Downloaded dra14338.dat\n",
      "Downloaded dra14339.dat\n",
      "Downloaded dra14340.dat\n",
      "Downloaded dra14341.dat\n",
      "Downloaded dra14342.dat\n",
      "Downloaded dra14343.dat\n",
      "Downloaded dra14344.dat\n",
      "Downloaded dra14345.dat\n",
      "Downloaded dra14346.dat\n",
      "Downloaded dra14347.dat\n",
      "Downloaded dra14348.dat\n",
      "Downloaded dra14349.dat\n",
      "Downloaded dra14350.dat\n",
      "Downloaded dra14351.dat\n",
      "Downloaded dra14352.dat\n",
      "Downloaded dra14353.dat\n",
      "Downloaded dra14354.dat\n",
      "Downloaded dra14355.dat\n",
      "Downloaded dra14356.dat\n",
      "Downloaded dra14357.dat\n",
      "Downloaded dra14358.dat\n",
      "Downloaded dra14359.dat\n",
      "Downloaded dra14360.dat\n",
      "Downloaded dra14361.dat\n",
      "Downloaded dra14362.dat\n",
      "Downloaded dra14363.dat\n",
      "Downloaded dra14364.dat\n",
      "Downloaded dra15001.dat\n",
      "Downloaded dra15002.dat\n",
      "Downloaded dra15003.dat\n",
      "Downloaded dra15004.dat\n",
      "Downloaded dra15005.dat\n",
      "Downloaded dra15006.dat\n",
      "Downloaded dra15007.dat\n",
      "Downloaded dra15008.dat\n",
      "Downloaded dra15009.dat\n",
      "Downloaded dra15010.dat\n",
      "Downloaded dra15011.dat\n",
      "Downloaded dra15012.dat\n",
      "Downloaded dra15013.dat\n",
      "Downloaded dra15014.dat\n",
      "Downloaded dra15015.dat\n",
      "Downloaded dra15016.dat\n",
      "Downloaded dra15017.dat\n",
      "Downloaded dra15018.dat\n",
      "Downloaded dra15019.dat\n",
      "Downloaded dra15020.dat\n",
      "Downloaded dra15021.dat\n",
      "Downloaded dra15022.dat\n",
      "Downloaded dra15023.dat\n",
      "Downloaded dra15024.dat\n",
      "Downloaded dra15025.dat\n",
      "Downloaded dra15026.dat\n",
      "Downloaded dra15027.dat\n",
      "Downloaded dra15028.dat\n",
      "Downloaded dra15029.dat\n",
      "Downloaded dra15030.dat\n",
      "Downloaded dra15031.dat\n",
      "Downloaded dra15032.dat\n",
      "Downloaded dra15033.dat\n",
      "Downloaded dra15034.dat\n",
      "Downloaded dra15035.dat\n",
      "Downloaded dra15036.dat\n",
      "Downloaded dra15037.dat\n",
      "Downloaded dra15038.dat\n",
      "Downloaded dra15039.dat\n",
      "Downloaded dra15040.dat\n",
      "Downloaded dra15041.dat\n",
      "Downloaded dra15042.dat\n",
      "Downloaded dra15043.dat\n",
      "Downloaded dra15044.dat\n",
      "Downloaded dra15045.dat\n",
      "Downloaded dra15046.dat\n",
      "Downloaded dra15047.dat\n",
      "Downloaded dra15048.dat\n",
      "Downloaded dra15049.dat\n",
      "Downloaded dra15050.dat\n",
      "Downloaded dra15051.dat\n",
      "Downloaded dra15052.dat\n",
      "Downloaded dra15053.dat\n",
      "Downloaded dra15054.dat\n",
      "Downloaded dra15055.dat\n",
      "Downloaded dra15056.dat\n",
      "Downloaded dra15057.dat\n",
      "Downloaded dra15058.dat\n",
      "Downloaded dra15059.dat\n",
      "Downloaded dra15060.dat\n",
      "Downloaded dra15061.dat\n",
      "Downloaded dra15062.dat\n",
      "Downloaded dra15063.dat\n",
      "Downloaded dra15064.dat\n",
      "Downloaded dra15065.dat\n",
      "Downloaded dra15066.dat\n",
      "Downloaded dra15067.dat\n",
      "Downloaded dra15068.dat\n",
      "Downloaded dra15069.dat\n",
      "Downloaded dra15070.dat\n",
      "Downloaded dra15071.dat\n",
      "Downloaded dra15072.dat\n",
      "Downloaded dra15073.dat\n",
      "Downloaded dra15074.dat\n",
      "Downloaded dra15075.dat\n",
      "Downloaded dra15076.dat\n",
      "Downloaded dra15077.dat\n",
      "Downloaded dra15078.dat\n",
      "Downloaded dra15079.dat\n",
      "Downloaded dra15080.dat\n",
      "Downloaded dra15081.dat\n",
      "Downloaded dra15082.dat\n",
      "Downloaded dra15083.dat\n",
      "Downloaded dra15084.dat\n",
      "Downloaded dra15085.dat\n",
      "Downloaded dra15086.dat\n",
      "Downloaded dra15087.dat\n",
      "Downloaded dra15088.dat\n",
      "Downloaded dra15089.dat\n",
      "Downloaded dra15090.dat\n",
      "Downloaded dra15091.dat\n",
      "Downloaded dra15092.dat\n",
      "Downloaded dra15093.dat\n",
      "Downloaded dra15094.dat\n",
      "Downloaded dra15095.dat\n",
      "Downloaded dra15096.dat\n",
      "Downloaded dra15097.dat\n",
      "Downloaded dra15098.dat\n",
      "Downloaded dra15099.dat\n",
      "Downloaded dra15100.dat\n",
      "Downloaded dra15101.dat\n",
      "Downloaded dra15102.dat\n",
      "Downloaded dra15103.dat\n",
      "Downloaded dra15104.dat\n",
      "Downloaded dra15105.dat\n",
      "Downloaded dra15106.dat\n",
      "Downloaded dra15107.dat\n",
      "Downloaded dra15108.dat\n",
      "Downloaded dra15109.dat\n",
      "Downloaded dra15110.dat\n",
      "Downloaded dra15111.dat\n",
      "Downloaded dra15112.dat\n",
      "Downloaded dra15113.dat\n",
      "Downloaded dra15114.dat\n",
      "Downloaded dra15115.dat\n",
      "Downloaded dra15116.dat\n",
      "Downloaded dra15117.dat\n",
      "Downloaded dra15118.dat\n",
      "Downloaded dra15119.dat\n",
      "Downloaded dra15120.dat\n",
      "Downloaded dra15121.dat\n",
      "Downloaded dra15122.dat\n",
      "Downloaded dra15123.dat\n",
      "Downloaded dra15124.dat\n",
      "Downloaded dra15125.dat\n",
      "Downloaded dra15126.dat\n",
      "Downloaded dra15127.dat\n",
      "Downloaded dra15128.dat\n",
      "Downloaded dra15129.dat\n",
      "Downloaded dra15130.dat\n",
      "Downloaded dra15131.dat\n",
      "Downloaded dra15132.dat\n",
      "Downloaded dra15133.dat\n",
      "Downloaded dra15134.dat\n",
      "Downloaded dra15135.dat\n",
      "Downloaded dra15136.dat\n",
      "Downloaded dra15137.dat\n",
      "Downloaded dra15138.dat\n",
      "Downloaded dra15139.dat\n",
      "Downloaded dra15140.dat\n",
      "Downloaded dra15141.dat\n",
      "Downloaded dra15142.dat\n",
      "Downloaded dra15143.dat\n",
      "Downloaded dra15144.dat\n",
      "Downloaded dra15145.dat\n",
      "Downloaded dra15146.dat\n",
      "Downloaded dra15147.dat\n",
      "Downloaded dra15148.dat\n",
      "Downloaded dra15149.dat\n",
      "Downloaded dra15150.dat\n",
      "Downloaded dra15151.dat\n",
      "Downloaded dra15152.dat\n",
      "Downloaded dra15153.dat\n",
      "Downloaded dra15154.dat\n",
      "Downloaded dra15155.dat\n",
      "Downloaded dra15156.dat\n",
      "Downloaded dra15157.dat\n",
      "Downloaded dra15158.dat\n",
      "Downloaded dra15159.dat\n",
      "Downloaded dra15160.dat\n",
      "Downloaded dra15161.dat\n",
      "Downloaded dra15162.dat\n",
      "Downloaded dra15163.dat\n",
      "Downloaded dra15164.dat\n",
      "Downloaded dra15165.dat\n",
      "Downloaded dra15166.dat\n",
      "Downloaded dra15167.dat\n",
      "Downloaded dra15168.dat\n",
      "Downloaded dra15169.dat\n",
      "Downloaded dra15170.dat\n",
      "Downloaded dra15171.dat\n",
      "Downloaded dra15172.dat\n",
      "Downloaded dra15173.dat\n",
      "Downloaded dra15174.dat\n",
      "Downloaded dra15175.dat\n",
      "Downloaded dra15176.dat\n",
      "Downloaded dra15177.dat\n",
      "Downloaded dra15178.dat\n",
      "Downloaded dra15179.dat\n",
      "Downloaded dra15180.dat\n",
      "Downloaded dra15181.dat\n",
      "Downloaded dra15182.dat\n",
      "Downloaded dra15183.dat\n",
      "Downloaded dra15184.dat\n",
      "Downloaded dra15185.dat\n",
      "Downloaded dra15186.dat\n",
      "Downloaded dra15187.dat\n",
      "Downloaded dra15188.dat\n",
      "Downloaded dra15189.dat\n",
      "Downloaded dra15190.dat\n",
      "Downloaded dra15191.dat\n",
      "Downloaded dra15192.dat\n",
      "Downloaded dra15193.dat\n",
      "Downloaded dra15194.dat\n",
      "Downloaded dra15195.dat\n",
      "Downloaded dra15196.dat\n",
      "Downloaded dra15197.dat\n",
      "Downloaded dra15198.dat\n",
      "Downloaded dra15199.dat\n",
      "Downloaded dra15200.dat\n",
      "Downloaded dra15201.dat\n",
      "Downloaded dra15202.dat\n",
      "Downloaded dra15203.dat\n",
      "Downloaded dra15204.dat\n",
      "Downloaded dra15205.dat\n",
      "Downloaded dra15206.dat\n",
      "Downloaded dra15207.dat\n",
      "Downloaded dra15208.dat\n",
      "Downloaded dra15209.dat\n",
      "Downloaded dra15210.dat\n",
      "Downloaded dra15211.dat\n",
      "Downloaded dra15212.dat\n",
      "Downloaded dra15213.dat\n",
      "Downloaded dra15214.dat\n",
      "Downloaded dra15215.dat\n",
      "Downloaded dra15216.dat\n",
      "Downloaded dra15217.dat\n",
      "Downloaded dra15218.dat\n",
      "Downloaded dra15219.dat\n",
      "Downloaded dra15220.dat\n",
      "Downloaded dra15221.dat\n",
      "Downloaded dra15222.dat\n",
      "Downloaded dra15223.dat\n",
      "Downloaded dra15224.dat\n",
      "Downloaded dra15225.dat\n",
      "Downloaded dra15226.dat\n",
      "Downloaded dra15227.dat\n",
      "Downloaded dra15228.dat\n",
      "Downloaded dra15229.dat\n",
      "Downloaded dra15230.dat\n",
      "Downloaded dra15231.dat\n",
      "Downloaded dra15232.dat\n",
      "Downloaded dra15233.dat\n",
      "Downloaded dra15234.dat\n",
      "Downloaded dra15235.dat\n",
      "Downloaded dra15236.dat\n",
      "Downloaded dra15237.dat\n",
      "Downloaded dra15238.dat\n",
      "Downloaded dra15239.dat\n",
      "Downloaded dra15240.dat\n",
      "Downloaded dra15241.dat\n",
      "Downloaded dra15242.dat\n",
      "Downloaded dra15243.dat\n",
      "Downloaded dra15244.dat\n",
      "Downloaded dra15245.dat\n",
      "Downloaded dra15246.dat\n",
      "Downloaded dra15247.dat\n",
      "Downloaded dra15248.dat\n",
      "Downloaded dra15249.dat\n",
      "Downloaded dra15250.dat\n",
      "Downloaded dra15251.dat\n",
      "Downloaded dra15252.dat\n",
      "Downloaded dra15253.dat\n",
      "Downloaded dra15254.dat\n",
      "Downloaded dra15255.dat\n",
      "Downloaded dra15256.dat\n",
      "Downloaded dra15257.dat\n",
      "Downloaded dra15258.dat\n",
      "Downloaded dra15259.dat\n",
      "Downloaded dra15260.dat\n",
      "Downloaded dra15261.dat\n",
      "Downloaded dra15262.dat\n",
      "Downloaded dra15263.dat\n",
      "Downloaded dra15264.dat\n",
      "Downloaded dra15265.dat\n",
      "Downloaded dra15266.dat\n",
      "Downloaded dra15267.dat\n",
      "Downloaded dra15268.dat\n",
      "Downloaded dra15269.dat\n",
      "Downloaded dra15270.dat\n",
      "Downloaded dra15271.dat\n",
      "Downloaded dra15272.dat\n",
      "Downloaded dra15273.dat\n",
      "Downloaded dra15274.dat\n",
      "Downloaded dra15275.dat\n",
      "Downloaded dra15276.dat\n",
      "Downloaded dra15277.dat\n",
      "Downloaded dra15278.dat\n",
      "Downloaded dra15279.dat\n",
      "Downloaded dra15280.dat\n",
      "Downloaded dra15281.dat\n",
      "Downloaded dra15282.dat\n",
      "Downloaded dra15283.dat\n",
      "Downloaded dra15284.dat\n",
      "Downloaded dra15285.dat\n",
      "Downloaded dra15286.dat\n",
      "Downloaded dra15287.dat\n",
      "Downloaded dra15288.dat\n",
      "Downloaded dra15289.dat\n",
      "Downloaded dra15290.dat\n",
      "Downloaded dra15291.dat\n",
      "Downloaded dra15292.dat\n",
      "Downloaded dra15293.dat\n",
      "Downloaded dra15294.dat\n",
      "Downloaded dra15295.dat\n",
      "Downloaded dra15296.dat\n",
      "Downloaded dra15297.dat\n",
      "Downloaded dra15298.dat\n",
      "Downloaded dra15299.dat\n",
      "Downloaded dra15300.dat\n",
      "Downloaded dra15301.dat\n",
      "Downloaded dra15302.dat\n",
      "Downloaded dra15303.dat\n",
      "Downloaded dra15304.dat\n",
      "Downloaded dra15305.dat\n",
      "Downloaded dra15306.dat\n",
      "Downloaded dra15307.dat\n",
      "Downloaded dra15308.dat\n",
      "Downloaded dra15309.dat\n",
      "Downloaded dra15310.dat\n",
      "Downloaded dra15311.dat\n",
      "Downloaded dra15312.dat\n",
      "Downloaded dra15313.dat\n",
      "Downloaded dra15314.dat\n",
      "Downloaded dra15315.dat\n",
      "Downloaded dra15316.dat\n",
      "Downloaded dra15317.dat\n",
      "Downloaded dra15318.dat\n",
      "Downloaded dra15319.dat\n",
      "Downloaded dra15320.dat\n",
      "Downloaded dra15321.dat\n",
      "Downloaded dra15322.dat\n",
      "Downloaded dra15323.dat\n",
      "Downloaded dra15324.dat\n",
      "Downloaded dra15325.dat\n",
      "Downloaded dra15326.dat\n",
      "Downloaded dra15327.dat\n",
      "Downloaded dra15328.dat\n",
      "Downloaded dra15329.dat\n",
      "Downloaded dra15330.dat\n",
      "Downloaded dra15331.dat\n",
      "Downloaded dra15332.dat\n",
      "Downloaded dra15333.dat\n",
      "Downloaded dra15334.dat\n",
      "Downloaded dra15335.dat\n",
      "Downloaded dra15336.dat\n",
      "Downloaded dra15337.dat\n",
      "Downloaded dra15338.dat\n",
      "Downloaded dra15339.dat\n",
      "Downloaded dra15340.dat\n",
      "Downloaded dra15341.dat\n",
      "Downloaded dra15342.dat\n",
      "Downloaded dra15343.dat\n",
      "Downloaded dra15344.dat\n",
      "Downloaded dra15345.dat\n",
      "Downloaded dra15346.dat\n",
      "Downloaded dra15347.dat\n",
      "Downloaded dra15348.dat\n",
      "Downloaded dra15349.dat\n",
      "Downloaded dra15350.dat\n",
      "Downloaded dra15351.dat\n",
      "Downloaded dra15352.dat\n",
      "Downloaded dra15353.dat\n",
      "Downloaded dra15354.dat\n",
      "Downloaded dra15355.dat\n",
      "Downloaded dra15356.dat\n",
      "Downloaded dra15357.dat\n",
      "Downloaded dra15358.dat\n",
      "Downloaded dra15359.dat\n",
      "Downloaded dra15360.dat\n",
      "Downloaded dra15361.dat\n",
      "Downloaded dra15362.dat\n",
      "Downloaded dra15363.dat\n",
      "Downloaded dra15364.dat\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T17:14:46.845430Z",
     "start_time": "2025-03-25T17:14:45.675466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_date_labels(station='dra'):\n",
    "\n",
    "    base_url = \"https://sheridan.geog.kent.edu/ssc/files/\"\n",
    "    file_name = station.upper() + \".cal3\"\n",
    "\n",
    "    try:\n",
    "        url = f\"{base_url}{file_name}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(file_name, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Downloaded {file_name}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {file_name}: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {file_name}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "create_date_labels(station='dra')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded DRA.cal3\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T17:50:25.814706Z",
     "start_time": "2025-03-25T17:50:25.798164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_labels(station='dra', start_date='2014-01-01', end_date='2015-12-31'):\n",
    "\n",
    "    file_name = station.upper() + \".cal3\"\n",
    "\n",
    "    a = pd.read_csv(file_name, sep='\\s+', header=None)\n",
    "    selection = a.loc[(a[1] >= int(start_date.replace(\"-\", \"\"))) & (a[1] <= int(end_date.replace(\"-\", \"\")))]\n",
    "    return selection[2].divide(10).round(0).astype(int).to_numpy()  # remove '+' labels\n",
    "\n",
    "labels = read_labels()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T17:50:40.045066Z",
     "start_time": "2025-03-25T17:50:28.216326Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def read_surfrad_data(data_dir):\n",
    "    \"\"\"\n",
    "    Read all SURFRAD .dat files in the specified directory\n",
    "    Returns a DataFrame with parsed data\n",
    "    \"\"\"\n",
    "    # Get all .dat files in the directory\n",
    "    file_pattern = os.path.join(data_dir, '*.dat')\n",
    "    data_files = glob.glob(file_pattern)\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for file_path in data_files:\n",
    "        try:\n",
    "            # SURFRAD files have a specific format with headers\n",
    "            # Skip the header lines (first 2 rows)\n",
    "            df = pd.read_csv(file_path, sep='\\s+', skiprows=2, header=None)\n",
    "            \n",
    "            # Extract filename for date information\n",
    "            \"\"\" filename = os.path.basename(file_path)\n",
    "            date_str = filename[3:11]  # Extract YYYYMMDD from draYYYYMMDD.dat\n",
    "            file_date = datetime.strptime(date_str, '%Y%m%d')\n",
    "            print(df[1]) \"\"\"\n",
    "        \n",
    "            \n",
    "            # Create timestamp from date and time columns\n",
    "            \"\"\" \n",
    "            # Filter out bad values (often marked as -9999.9)\n",
    "            parsed_df = parsed_df[parsed_df['irradiance'] > -999]\n",
    "            \n",
    "            all_data.append(parsed_df) \"\"\"\n",
    "            cols_to_drop = df.columns[9::2]  # Every second column from index 9 onward\n",
    "            df_cleaned = df.drop(columns=cols_to_drop)\n",
    "            all_data.append(df_cleaned)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    # Combine all data files\n",
    "    if not all_data:\n",
    "        raise ValueError(\"No valid data files found\")\n",
    "    \n",
    "    combined_data = pd.concat(all_data)\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    #combined_data = combined_data.sort_values('timestamp')\n",
    "    \n",
    "    # Set timestamp as index\n",
    "    #combined_data.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "# Load your NOAA SURFRAD data\n",
    "data_dir = 'final_project\\\\surfrad_data\\\\dra'  \n",
    "data_dir = os.path.abspath('surfrad_data/dra')\n",
    "df = read_surfrad_data(data_dir)\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "\n",
    "# Convert to 15-minute resolution by averaging\n",
    "def convert_to_15min_resolution(df):\n",
    "    # Ensure the DataFrame index is a range index\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Group the DataFrame by every 15 rows and calculate the mean for each group\n",
    "    df_15min = df.groupby(df.index // 15).mean()\n",
    "    \n",
    "    return df_15min\n",
    "\n",
    "# Now we can continue with the rest of your pipeline\n",
    "df_15min = convert_to_15min_resolution(df)\n",
    "\n",
    "# Extract only points 18-78 for each day (corresponding to daylight hours)\n",
    "def extract_daylight_hours(df):\n",
    "    # Define chunk size and slice range\n",
    "    chunk_size = 96\n",
    "    start, end = 18, 79  # Keep indices 18 to 78 (end index is exclusive)\n",
    "\n",
    "    # Process data in chunks\n",
    "    filtered_data = pd.concat([df.iloc[i + start : i + end] for i in range(0, len(df), chunk_size)])\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "df_daylight = extract_daylight_hours(df_15min)\n",
    "\n",
    "# Split into training and testing sets\n",
    "def split_train_test(df, lb):\n",
    "    data_points_per_day = 61\n",
    "    total_days = len(df) // data_points_per_day\n",
    "    labels_for_each_datapoint = np.repeat(lb, data_points_per_day)\n",
    "\n",
    "    train_days = int(total_days * 0.6)  # 70% for training\n",
    "    test_days = total_days - train_days\n",
    "\n",
    "    train_index = train_days * data_points_per_day\n",
    "    test_index = train_index + (test_days * data_points_per_day)\n",
    "\n",
    "    df_train = df.iloc[:train_index].reset_index(drop=True)\n",
    "    df_test = df.iloc[train_index:test_index].reset_index(drop=True)\n",
    "\n",
    "    labels_train = labels_for_each_datapoint[:train_index]\n",
    "    labels_test = labels_for_each_datapoint[train_index:test_index]\n",
    "\n",
    "    return df_train, df_test, labels_train, labels_test\n",
    "\n",
    "print(df_daylight)\n",
    "train_data, test_data, train_labels, test_labels = split_train_test(df_daylight, labels)\n",
    "print(train_data)\n",
    "\n",
    "# Extract the irradiance values as numpy arrays for model training\n",
    "print(train_data[16].values)\n",
    "X_train = train_data[16].values.reshape(-1, 61)  # 61 points per day (18-78)\n",
    "X_test = test_data[16].values.reshape(-1, 61) # look up what the right index is for the dataset \n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "print(X_train)\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training data shape: {X_train_normalized.shape}\")\n",
    "print(f\"Testing data shape: {X_test_normalized.shape}\")\n",
    "\n",
    "print(f\"Training labels shape: {train_labels.shape}\")\n",
    "print(f\"Testing labels shape: {test_labels.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0      1     2     3          4     5          6           7   \\\n",
      "18     2014.0    1.0   1.0   1.0   4.000000  37.0   4.616667  136.880000   \n",
      "19     2014.0    1.0   1.0   1.0   4.000000  52.0   4.866667  139.868000   \n",
      "20     2014.0    1.0   1.0   1.0   5.000000   7.0   5.116667  142.842667   \n",
      "21     2014.0    1.0   1.0   1.0   5.000000  22.0   5.366667  145.790000   \n",
      "22     2014.0    1.0   1.0   1.0   5.000000  37.0   5.616667  148.696000   \n",
      "...       ...    ...   ...   ...        ...   ...        ...         ...   \n",
      "69844  2015.0  364.0  12.0  30.0  22.733333  19.0  23.050000   75.454667   \n",
      "69845  2015.0  364.0  12.0  30.0  23.000000  18.0  23.300000   77.650000   \n",
      "69846  2015.0  364.0  12.0  30.0  23.000000  33.0  23.550000   79.929333   \n",
      "69847  2015.0  364.0  12.0  30.0  23.000000  48.0  23.800000   82.284667   \n",
      "69848  2015.0  364.0  12.0  30.0  23.000000  57.5  23.958250   83.810000   \n",
      "\n",
      "               8          10          12         14          16          18  \\\n",
      "18      -4.160000  -0.866667   -0.240000   0.000000  253.973333  279.873333   \n",
      "19      -4.060000  -0.420000    0.493333  -0.013333  253.493333  279.726667   \n",
      "20      -3.853333  -0.506667    1.106667  -0.073333  252.440000  279.433333   \n",
      "21      -3.620000  -0.626667    0.853333  -0.133333  250.446667  278.626667   \n",
      "22      -3.653333  -0.566667    0.673333  -0.106667  250.033333  278.040000   \n",
      "...           ...        ...         ...        ...         ...         ...   \n",
      "69844  219.480000  59.473333  831.766667  41.286667  231.666667  283.660000   \n",
      "69845  177.973333  50.420000  788.446667  38.186667  232.106667  283.506667   \n",
      "69846  136.986667  41.240000  728.440000  33.980000  231.733333  283.273333   \n",
      "69847   97.193333  31.473333  654.926667  29.266667  231.240000  282.800000   \n",
      "69848   71.675000  25.350000  594.175000  25.675000  231.100000  282.475000   \n",
      "\n",
      "               20          22          24          26         28          30  \\\n",
      "18     279.593333  323.773333  279.386667  279.420000   0.000000    0.013333   \n",
      "19     279.453333  322.393333  279.393333  279.393333   0.000000    0.000000   \n",
      "20     279.106667  320.126667  279.026667  279.006667   0.000000    0.000000   \n",
      "21     278.293333  317.680000  278.493333  278.460000   0.000000    0.000000   \n",
      "22     277.773333  316.680000  277.820000  277.820000   0.000000    0.000000   \n",
      "...           ...         ...         ...         ...        ...         ...   \n",
      "69844  282.980000  366.546667  281.300000  281.400000  10.660000  102.106667   \n",
      "69845  282.833333  362.260000  281.273333  281.346667   7.913333   83.966667   \n",
      "69846  282.586667  356.080000  281.000000  281.040000   5.486667   65.960000   \n",
      "69847  282.173333  350.026667  280.700000  280.713333   3.566667   47.926667   \n",
      "69848  281.850000  345.850000  280.400000  280.400000   2.550000   35.300000   \n",
      "\n",
      "               32          34         36        38         40        42  \\\n",
      "18       0.000000  -69.793333 -69.793333  8.493333  23.166667  0.833333   \n",
      "19       0.000000  -68.886667 -68.886667  8.073333  24.000000  0.280000   \n",
      "20       0.000000  -67.706667 -67.706667  6.940000  25.846667  0.340000   \n",
      "21       0.000000  -67.213333 -67.213333  5.486667  28.893333  0.306667   \n",
      "22       0.000000  -66.653333 -66.653333  4.780000  30.053333  0.340000   \n",
      "...           ...         ...        ...       ...        ...       ...   \n",
      "69844  190.800000 -134.866667  55.906667  7.426667  28.060000  1.420000   \n",
      "69845  156.586667 -130.153333  26.433333  7.753333  27.546667  1.626667   \n",
      "69846  120.320000 -124.333333  -4.026667  7.213333  28.593333  1.920000   \n",
      "69847   86.013333 -118.793333 -32.786667  7.046667  28.920000  2.160000   \n",
      "69848   64.450000 -114.725000 -50.275000  6.800000  29.450000  2.975000   \n",
      "\n",
      "               44          46  \n",
      "18      56.353333  908.693333  \n",
      "19     147.726667  908.840000  \n",
      "20     313.520000  908.813333  \n",
      "21     279.853333  908.826667  \n",
      "22     315.906667  908.900000  \n",
      "...           ...         ...  \n",
      "69844  244.213333  906.393333  \n",
      "69845  234.700000  906.493333  \n",
      "69846  199.466667  906.500000  \n",
      "69847  194.200000  906.413333  \n",
      "69848  207.875000  906.300000  \n",
      "\n",
      "[44386 rows x 28 columns]\n",
      "           0     1    2     3         4     5         6           7   \\\n",
      "0      2014.0   1.0  1.0   1.0  4.000000  37.0  4.616667  136.880000   \n",
      "1      2014.0   1.0  1.0   1.0  4.000000  52.0  4.866667  139.868000   \n",
      "2      2014.0   1.0  1.0   1.0  5.000000   7.0  5.116667  142.842667   \n",
      "3      2014.0   1.0  1.0   1.0  5.000000  22.0  5.366667  145.790000   \n",
      "4      2014.0   1.0  1.0   1.0  5.000000  37.0  5.616667  148.696000   \n",
      "...       ...   ...  ...   ...       ...   ...       ...         ...   \n",
      "26591  2015.0  73.0  3.0  14.0  3.000000  12.0  3.200000  107.200667   \n",
      "26592  2015.0  73.0  3.0  14.0  3.000000  27.0  3.450000  110.154000   \n",
      "26593  2015.0  73.0  3.0  14.0  3.000000  42.0  3.700000  113.078667   \n",
      "26594  2015.0  73.0  3.0  14.0  3.333333  37.0  3.950000  115.970000   \n",
      "26595  2015.0  73.0  3.0  14.0  4.000000  12.0  4.200000  118.818667   \n",
      "\n",
      "             8         10        12        14          16          18  \\\n",
      "0     -4.160000 -0.866667 -0.240000  0.000000  253.973333  279.873333   \n",
      "1     -4.060000 -0.420000  0.493333 -0.013333  253.493333  279.726667   \n",
      "2     -3.853333 -0.506667  1.106667 -0.073333  252.440000  279.433333   \n",
      "3     -3.620000 -0.626667  0.853333 -0.133333  250.446667  278.626667   \n",
      "4     -3.653333 -0.566667  0.673333 -0.106667  250.033333  278.040000   \n",
      "...         ...       ...       ...       ...         ...         ...   \n",
      "26591 -4.960000 -0.040000 -0.373333 -0.400000  290.333333  291.633333   \n",
      "26592 -5.300000 -0.146667 -0.280000 -0.400000  290.753333  291.373333   \n",
      "26593 -4.913333 -0.526667 -0.400000 -0.400000  291.193333  291.073333   \n",
      "26594 -4.853333 -0.600000 -0.433333 -0.433333  290.793333  290.833333   \n",
      "26595 -4.800000 -0.626667 -1.193333 -0.426667  288.026667  290.086667   \n",
      "\n",
      "               20          22          24          26   28        30   32  \\\n",
      "0      279.593333  323.773333  279.386667  279.420000  0.0  0.013333  0.0   \n",
      "1      279.453333  322.393333  279.393333  279.393333  0.0  0.000000  0.0   \n",
      "2      279.106667  320.126667  279.026667  279.006667  0.0  0.000000  0.0   \n",
      "3      278.293333  317.680000  278.493333  278.460000  0.0  0.000000  0.0   \n",
      "4      277.773333  316.680000  277.820000  277.820000  0.0  0.000000  0.0   \n",
      "...           ...         ...         ...         ...  ...       ...  ...   \n",
      "26591  291.180000  388.520000  291.260000  291.213333  0.0  0.000000  0.0   \n",
      "26592  290.913333  387.500000  290.826667  290.786667  0.0  0.000000  0.0   \n",
      "26593  290.626667  385.080000  290.486667  290.466667  0.0  0.000000  0.0   \n",
      "26594  290.406667  382.993333  290.406667  290.386667  0.0  0.000000  0.0   \n",
      "26595  289.566667  379.373333  289.960000  289.920000  0.0  0.000000  0.0   \n",
      "\n",
      "              34         36         38         40        42          44  \\\n",
      "0     -69.793333 -69.793333   8.493333  23.166667  0.833333   56.353333   \n",
      "1     -68.886667 -68.886667   8.073333  24.000000  0.280000  147.726667   \n",
      "2     -67.706667 -67.706667   6.940000  25.846667  0.340000  313.520000   \n",
      "3     -67.213333 -67.213333   5.486667  28.893333  0.306667  279.853333   \n",
      "4     -66.653333 -66.653333   4.780000  30.053333  0.340000  315.906667   \n",
      "...          ...        ...        ...        ...       ...         ...   \n",
      "26591 -98.160000 -98.160000  18.560000  21.753333  3.680000   95.966667   \n",
      "26592 -96.760000 -96.760000  18.160000  22.480000  3.713333   95.786667   \n",
      "26593 -93.886667 -93.886667  18.100000  22.493333  3.353333   93.060000   \n",
      "26594 -92.186667 -92.186667  18.280000  22.286667  2.613333   92.146667   \n",
      "26595 -91.360000 -91.360000  16.946667  24.893333  0.446667  238.240000   \n",
      "\n",
      "               46  \n",
      "0      908.693333  \n",
      "1      908.840000  \n",
      "2      908.813333  \n",
      "3      908.826667  \n",
      "4      908.900000  \n",
      "...           ...  \n",
      "26591  908.113333  \n",
      "26592  908.320000  \n",
      "26593  908.426667  \n",
      "26594  908.580000  \n",
      "26595  908.726667  \n",
      "\n",
      "[26596 rows x 28 columns]\n",
      "[253.97333333 253.49333333 252.44       ... 291.19333333 290.79333333\n",
      " 288.02666667]\n",
      "[[253.97333333 253.49333333 252.44       ... 277.59333333 278.6\n",
      "  280.17333333]\n",
      " [268.52       267.86       265.88       ... 272.66666667 273.94\n",
      "  274.62      ]\n",
      " [256.4        256.86666667 256.65333333 ... 267.16       269.18\n",
      "  270.28666667]\n",
      " ...\n",
      " [337.08       332.18666667 335.90666667 ... 351.18666667 343.01333333\n",
      "  339.29333333]\n",
      " [279.38       280.22666667 277.80666667 ... 297.1        296.53333333\n",
      "  295.02666667]\n",
      " [272.06666667 269.84666667 270.44       ... 291.19333333 290.79333333\n",
      "  288.02666667]]\n",
      "Training data shape: (436, 61)\n",
      "Testing data shape: (291, 61)\n",
      "Training labels shape: (26596,)\n",
      "Testing labels shape: (17751,)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modify the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T17:54:32.873356Z",
     "start_time": "2025-03-25T17:54:32.661289Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Update the model architecture based on paper's specifications\n",
    "def update_model_architecture(model):\n",
    "    # Generator and discriminator with 2 hidden layers of 124 neurons each\n",
    "    generator = models.Sequential([\n",
    "        layers.Dense(124, activation='relu', input_shape=(60,)),  # 60-dim uniform noise\n",
    "        layers.Dense(124, activation='relu'),\n",
    "        layers.Dense(61)  # Output layer (61 points for positions 18-78)\n",
    "    ])\n",
    "    \n",
    "    discriminator = models.Sequential([\n",
    "        layers.Dense(124, activation='relu', input_shape=(61,)),\n",
    "        layers.Dense(124, activation='relu'),\n",
    "        layers.Dense(num_of_classes)  # 10 classes output\n",
    "    ])\n",
    "    \n",
    "    # Update model properties\n",
    "    model.generator = generator\n",
    "    model.discriminator = discriminator\n",
    "    model.latent_dim = 60\n",
    "    model.generator_optimizer = tf.keras.optimizers.Adam()\n",
    "    model.discriminator_optimizer = tf.keras.optimizers.Adam()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Update your existing model\n",
    "wgan_model = WeatherClassificationWGANGP(num_classes=num_of_classes)\n",
    "wgan_model = update_model_architecture(wgan_model)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I:\\PyPoetry\\Cache\\virtualenvs\\gnn-final-project--Eia0XWR-py3.11\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "I:\\PyPoetry\\Cache\\virtualenvs\\gnn-final-project--Eia0XWR-py3.11\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n",
      "I:\\PyPoetry\\Cache\\virtualenvs\\gnn-final-project--Eia0XWR-py3.11\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modify the Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T18:04:03.200664Z",
     "start_time": "2025-03-25T18:04:03.195536Z"
    }
   },
   "source": [
    "def modified_train(model, dataset, epochs, batch_size=64):\n",
    "    \"\"\"Modified training procedure to match paper specifications\"\"\"\n",
    "    steps_per_epoch = len(dataset) // batch_size\n",
    "    \n",
    "    disc_losses = []\n",
    "    gen_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_disc_losses = []\n",
    "        epoch_gen_losses = []\n",
    "        \n",
    "        for step in range(steps_per_epoch):\n",
    "            # Get real images batch\n",
    "            indices = np.random.randint(0, len(dataset), batch_size)\n",
    "            real_data = dataset.iloc[indices]\n",
    "            print(real_data.shape)\n",
    "            print(real_data.shape)\n",
    "            # Get weather class labels based on your classification scheme\n",
    "            # labels = np.zeros(batch_size)  # Replace with actual labels\n",
    "            labels = train_labels[indices]\n",
    "            print(labels.shape)\n",
    "\n",
    "            # Train discriminator for 15 steps (as per paper)\n",
    "            for _ in range(15):\n",
    "                d_loss, real_loss, fake_loss, gp = model.train_discriminator(real_data, labels)\n",
    "                epoch_disc_losses.append(d_loss.numpy())\n",
    "            \n",
    "            # Train generator for 1 step\n",
    "            g_loss = model.train_generator()\n",
    "            epoch_gen_losses.append(g_loss.numpy())\n",
    "        \n",
    "        # Store and print losses\n",
    "        disc_losses.append(np.mean(epoch_disc_losses))\n",
    "        gen_losses.append(np.mean(epoch_gen_losses))\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"D loss: {disc_losses[-1]:.4f}, G loss: {gen_losses[-1]:.4f}\")\n",
    "    \n",
    "    return disc_losses, gen_losses"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Implement the Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T17:54:37.203039Z",
     "start_time": "2025-03-25T17:54:37.197033Z"
    }
   },
   "source": [
    "def build_cnn1d_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=(61, 1)),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(64, kernel_size=5, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(64, kernel_size=8, activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(num_of_classes, activation='softmax')  # 10 weather classes\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_cnn2d_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(64, kernel_size=(1, 1), activation='relu', input_shape=(61, 1, 1)),\n",
    "        layers.Conv2D(64, kernel_size=(2, 1), activation='relu'),\n",
    "        layers.Conv2D(64, kernel_size=(3, 2), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(num_of_classes, activation='softmax')  # 10 weather classes\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_mlp_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Flatten(input_shape=(61,)),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(num_of_classes, activation='softmax')  # 10 weather classes\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train SVM and KNN Models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T17:54:40.724904Z",
     "start_time": "2025-03-25T17:54:39.792654Z"
    }
   },
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def train_svm_model(X_train, y_train):\n",
    "    # Grid search for optimal parameters\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': [0.001, 0.01, 0.1, 1],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "    \n",
    "    svm = SVC()\n",
    "    grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best SVM parameters: {grid_search.best_params_}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def train_knn_model(X_train, y_train):\n",
    "    # Grid search for optimal parameters\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan']\n",
    "    }\n",
    "    \n",
    "    knn = KNeighborsClassifier()\n",
    "    grid_search = GridSearchCV(knn, param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best KNN parameters: {grid_search.best_params_}\")\n",
    "    return grid_search.best_estimator_"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Complete Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T17:54:43.981780Z",
     "start_time": "2025-03-25T17:54:43.975782Z"
    }
   },
   "source": [
    "def train_full_pipeline(train_data, test_data, epochs=100):\n",
    "    # Step 1: Train WGAN-GP model\n",
    "    wgan_model = WeatherClassificationWGANGP(num_classes=num_of_classes)\n",
    "    wgan_model = update_model_architecture(wgan_model)\n",
    "    disc_losses, gen_losses = modified_train(wgan_model, train_data, epochs)\n",
    "    \n",
    "    # Step 2: Generate synthetic data\n",
    "    n_synthetic = 10000  # Define how many synthetic samples to generate\n",
    "    synthetic_data, synthetic_labels = wgan_model.generate_synthetic_data(n_synthetic)\n",
    "    \n",
    "    # Denormalize synthetic data\n",
    "    synthetic_data_denorm = scaler.inverse_transform(synthetic_data)\n",
    "    \n",
    "    # Step 3: Combine real and synthetic data\n",
    "    X_combined = np.vstack([train_data, synthetic_data])\n",
    "    y_combined = np.concatenate([train_labels, synthetic_labels])  # Assuming you have train_labels\n",
    "    \n",
    "    # Step 4: Train classification models\n",
    "    # CNN1D\n",
    "    cnn1d = build_cnn1d_model()\n",
    "    X_cnn1d = X_combined.reshape(-1, 61, 1)  # Reshape for 1D convolution\n",
    "    cnn1d.fit(X_cnn1d, y_combined, epochs=50, batch_size=32, validation_split=0.2)\n",
    "    \n",
    "    # CNN2D\n",
    "    cnn2d = build_cnn2d_model()\n",
    "    X_cnn2d = X_combined.reshape(-1, 61, 1, 1)  # Reshape for 2D convolution\n",
    "    cnn2d.fit(X_cnn2d, y_combined, epochs=50, batch_size=32, validation_split=0.2)\n",
    "    \n",
    "    # MLP\n",
    "    mlp = build_mlp_model()\n",
    "    mlp.fit(X_combined, y_combined, epochs=50, batch_size=32, validation_split=0.2)\n",
    "    \n",
    "    # SVM\n",
    "    svm = train_svm_model(X_combined, y_combined)\n",
    "    \n",
    "    # KNN\n",
    "    knn = train_knn_model(X_combined, y_combined)\n",
    "    \n",
    "    # Step 5: Evaluate models on test data\n",
    "    X_test_cnn1d = test_data.reshape(-1, 61, 1)\n",
    "    X_test_cnn2d = test_data.reshape(-1, 61, 1, 1)\n",
    "    \n",
    "    print(\"CNN1D Accuracy:\", cnn1d.evaluate(X_test_cnn1d, test_labels)[1])\n",
    "    print(\"CNN2D Accuracy:\", cnn2d.evaluate(X_test_cnn2d, test_labels)[1])\n",
    "    print(\"MLP Accuracy:\", mlp.evaluate(test_data, test_labels)[1])\n",
    "    print(\"SVM Accuracy:\", svm.score(test_data, test_labels))\n",
    "    print(\"KNN Accuracy:\", knn.score(test_data, test_labels))\n",
    "    \n",
    "    return {\n",
    "        'wgan_model': wgan_model,\n",
    "        'cnn1d': cnn1d,\n",
    "        'cnn2d': cnn2d,\n",
    "        'mlp': mlp,\n",
    "        'svm': svm,\n",
    "        'knn': knn\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T17:54:46.386187Z",
     "start_time": "2025-03-25T17:54:46.378304Z"
    }
   },
   "source": [
    "class CNN_Classifier:\n",
    "    \"\"\"Convolutional Neural Network for Weather Classification\"\"\"\n",
    "    def __init__(self, img_shape=(64, 64, 3), num_classes=10):\n",
    "        self.img_shape = img_shape\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Build model\n",
    "        self.model = self.build_model()\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Build CNN model for weather classification\"\"\"\n",
    "        model = models.Sequential([\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', input_shape=self.img_shape),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(self.num_classes)\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    def train(self, train_images, train_labels, validation_data=None, epochs=10, batch_size=32):\n",
    "        \"\"\"Train the CNN classifier\"\"\"\n",
    "        # Set up callbacks\n",
    "        checkpoint = ModelCheckpoint(\n",
    "            'model_checkpoints/cnn_classifier_best.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            train_images, train_labels,\n",
    "            validation_data=validation_data,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[checkpoint, early_stopping]\n",
    "        )\n",
    "        \n",
    "        # Plot training history\n",
    "        self.plot_training_history(history)\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def plot_training_history(self, history):\n",
    "        \"\"\"Plot and save training history\"\"\"\n",
    "        # Plot accuracy\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.title('CNN Classifier - Accuracy')\n",
    "        \n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('CNN Classifier - Loss')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('results/cnn_classifier_training_history.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def evaluate(self, test_images, test_labels):\n",
    "        \"\"\"Evaluate the CNN classifier\"\"\"\n",
    "        return self.model.evaluate(test_images, test_labels)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T18:07:29.952344Z",
     "start_time": "2025-03-25T18:07:29.942825Z"
    }
   },
   "cell_type": "code",
   "source": "wgan_model.discriminator.summary()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_3\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_5 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m124\u001B[0m)            │         \u001B[38;5;34m7,688\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m124\u001B[0m)            │        \u001B[38;5;34m15,500\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m10\u001B[0m)             │         \u001B[38;5;34m1,250\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,688</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,250</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m24,438\u001B[0m (95.46 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,438</span> (95.46 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m24,438\u001B[0m (95.46 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,438</span> (95.46 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T18:07:45.223154Z",
     "start_time": "2025-03-25T18:07:45.211829Z"
    }
   },
   "cell_type": "code",
   "source": "wgan_model.generator.summary()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_2\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m124\u001B[0m)            │         \u001B[38;5;34m7,564\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m124\u001B[0m)            │        \u001B[38;5;34m15,500\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m61\u001B[0m)             │         \u001B[38;5;34m7,625\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,564</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,625</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m30,689\u001B[0m (119.88 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,689</span> (119.88 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m30,689\u001B[0m (119.88 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,689</span> (119.88 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
