{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for saving results\n",
    "os.makedirs('generated_samples', exist_ok=True)\n",
    "os.makedirs('model_checkpoints', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherClassificationWGANGP:\n",
    "    def __init__(self, img_shape=(64, 64, 3), num_classes=10, latent_dim=100, gradient_penalty_weight=10):\n",
    "        self.img_shape = img_shape\n",
    "        self.num_classes = num_classes  # 10 types of weather status as mentioned in the paper\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gradient_penalty_weight = gradient_penalty_weight\n",
    "        \n",
    "        # Following WGAN-GP paper, we use Adam optimizer\n",
    "        self.generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0, beta_2=0.9)\n",
    "        self.discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0, beta_2=0.9)\n",
    "        \n",
    "        # Build models\n",
    "        self.generator = self.build_generator()\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        \n",
    "    def build_generator(self):\n",
    "        \"\"\"Build the generator model as described in the paper\"\"\"\n",
    "        model = models.Sequential()\n",
    "        \n",
    "        # Foundation for 4x4 feature maps\n",
    "        model.add(layers.Dense(4 * 4 * 256, use_bias=False, input_shape=(self.latent_dim,)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        model.add(layers.Reshape((4, 4, 256)))\n",
    "        \n",
    "        # Upsampling layers\n",
    "        model.add(layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(layers.Conv2DTranspose(self.img_shape[2], (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "        \"\"\"Build the discriminator model as described in the paper\"\"\"\n",
    "        model = models.Sequential()\n",
    "        \n",
    "        # No batch normalization in critic as per WGAN-GP paper\n",
    "        model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=self.img_shape))\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        model.add(layers.Dropout(0.3))\n",
    "        \n",
    "        model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        model.add(layers.Dropout(0.3))\n",
    "        \n",
    "        model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n",
    "        model.add(layers.LeakyReLU(alpha=0.2))\n",
    "        model.add(layers.Dropout(0.3))\n",
    "        \n",
    "        model.add(layers.Flatten())\n",
    "        \n",
    "        # Weather classification output\n",
    "        model.add(layers.Dense(self.num_classes))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def gradient_penalty(self, real_images, fake_images):\n",
    "        \"\"\"Calculate the gradient penalty for WGAN-GP\"\"\"\n",
    "        batch_size = real_images.shape[0]\n",
    "        \n",
    "        # Create random interpolation points between real and fake images\n",
    "        alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        interpolated = alpha * real_images + (1 - alpha) * fake_images\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(interpolated)\n",
    "            # Get critic output for interpolated images\n",
    "            interpolated_output = self.discriminator(interpolated, training=True)\n",
    "        \n",
    "        # Calculate gradients with respect to inputs\n",
    "        gradients = tape.gradient(interpolated_output, interpolated)\n",
    "        # Calculate norm of gradients\n",
    "        gradients_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n",
    "        # Calculate gradient penalty\n",
    "        gradient_penalty = tf.reduce_mean((gradients_norm - 1.0) ** 2)\n",
    "        \n",
    "        return gradient_penalty\n",
    "    \n",
    "    @tf.function\n",
    "    def train_discriminator(self, real_images, labels):\n",
    "        \"\"\"Train the discriminator (critic) following WGAN-GP approach\"\"\"\n",
    "        batch_size = real_images.shape[0]\n",
    "        noise = tf.random.normal([batch_size, self.latent_dim])\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images\n",
    "            fake_images = self.generator(noise, training=True)\n",
    "            \n",
    "            # Get critic output for real and fake images\n",
    "            real_output = self.discriminator(real_images, training=True)\n",
    "            fake_output = self.discriminator(fake_images, training=True)\n",
    "            \n",
    "            # Calculate Wasserstein loss\n",
    "            real_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=labels, logits=real_output))\n",
    "            fake_loss = tf.reduce_mean(tf.reduce_mean(fake_output, axis=1))\n",
    "            \n",
    "            # Calculate gradient penalty\n",
    "            gp = self.gradient_penalty(real_images, fake_images)\n",
    "            \n",
    "            # Total discriminator loss\n",
    "            disc_loss = real_loss + fake_loss + self.gradient_penalty_weight * gp\n",
    "        \n",
    "        # Get gradients and update weights\n",
    "        gradients = tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "        self.discriminator_optimizer.apply_gradients(\n",
    "            zip(gradients, self.discriminator.trainable_variables))\n",
    "        \n",
    "        return disc_loss, real_loss, fake_loss, gp\n",
    "    \n",
    "    @tf.function\n",
    "    def train_generator(self):\n",
    "        \"\"\"Train the generator following WGAN-GP approach\"\"\"\n",
    "        batch_size = 64  # Fixed batch size for generator\n",
    "        noise = tf.random.normal([batch_size, self.latent_dim])\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images\n",
    "            fake_images = self.generator(noise, training=True)\n",
    "            \n",
    "            # Get critic output for fake images\n",
    "            fake_output = self.discriminator(fake_images, training=True)\n",
    "            \n",
    "            # Calculate generator loss\n",
    "            gen_loss = -tf.reduce_mean(tf.reduce_mean(fake_output, axis=1))\n",
    "        \n",
    "        # Get gradients and update weights\n",
    "        gradients = tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "        self.generator_optimizer.apply_gradients(\n",
    "            zip(gradients, self.generator.trainable_variables))\n",
    "        \n",
    "        return gen_loss\n",
    "    \n",
    "    def train(self, dataset, epochs, batch_size=64, n_critic=5):\n",
    "        \"\"\"Train the WGAN-GP model\"\"\"\n",
    "        steps_per_epoch = len(dataset) // batch_size\n",
    "        \n",
    "        # Lists to store loss values for plotting\n",
    "        disc_losses = []\n",
    "        gen_losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_disc_losses = []\n",
    "            epoch_gen_losses = []\n",
    "            \n",
    "            for step in range(steps_per_epoch):\n",
    "                # Get real images batch\n",
    "                indices = np.random.randint(0, len(dataset), batch_size)\n",
    "                real_images = dataset[indices]\n",
    "                labels = np.random.randint(0, self.num_classes, batch_size)  # Random labels for demonstration\n",
    "                \n",
    "                # Train discriminator for n_critic iterations\n",
    "                for _ in range(n_critic):\n",
    "                    d_loss, real_loss, fake_loss, gp = self.train_discriminator(real_images, labels)\n",
    "                    epoch_disc_losses.append(d_loss.numpy())\n",
    "                \n",
    "                # Train generator\n",
    "                g_loss = self.train_generator()\n",
    "                epoch_gen_losses.append(g_loss.numpy())\n",
    "                \n",
    "            # Append average losses for this epoch\n",
    "            disc_losses.append(np.mean(epoch_disc_losses))\n",
    "            gen_losses.append(np.mean(epoch_gen_losses))\n",
    "            \n",
    "            # Print progress and generate samples\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "                print(f\"D loss: {disc_losses[-1]:.4f}, G loss: {gen_losses[-1]:.4f}\")\n",
    "                self.generate_and_save_images(epoch + 1)\n",
    "                \n",
    "                # Plot loss curves\n",
    "                self.plot_losses(disc_losses, gen_losses, epoch + 1)\n",
    "            \n",
    "            # Save model checkpoints\n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                self.generator.save(f'model_checkpoints/generator_epoch_{epoch+1}.h5')\n",
    "                self.discriminator.save(f'model_checkpoints/discriminator_epoch_{epoch+1}.h5')\n",
    "    \n",
    "    def generate_and_save_images(self, epoch):\n",
    "        \"\"\"Generate and save sample images\"\"\"\n",
    "        noise = tf.random.normal([16, self.latent_dim])\n",
    "        generated_images = self.generator(noise, training=False)\n",
    "        generated_images = (generated_images + 1) / 2.0  # Rescale to [0, 1]\n",
    "        \n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        for i in range(generated_images.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            plt.imshow(generated_images[i, :, :, :])\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.savefig(f'generated_samples/epoch_{epoch}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_losses(self, disc_losses, gen_losses, epoch):\n",
    "        \"\"\"Plot and save loss curves\"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(disc_losses, label='Discriminator Loss')\n",
    "        plt.plot(gen_losses, label='Generator Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title(f'WGAN-GP Training Losses (Epoch {epoch})')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/losses_epoch_{epoch}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def generate_synthetic_data(self, n_samples, labels=None):\n",
    "        \"\"\"Generate synthetic weather images\"\"\"\n",
    "        noise = tf.random.normal([n_samples, self.latent_dim])\n",
    "        generated_images = self.generator(noise, training=False)\n",
    "        \n",
    "        # If labels are not provided, generate random labels\n",
    "        if labels is None:\n",
    "            labels = np.random.randint(0, self.num_classes, n_samples)\n",
    "        \n",
    "        return generated_images, labels\n",
    "    \n",
    "    def classify_weather(self, images):\n",
    "        \"\"\"Classify weather images using the trained discriminator\"\"\"\n",
    "        logits = self.discriminator(images, training=False)\n",
    "        return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded dra14001.dat\n",
      "Downloaded dra14002.dat\n",
      "Downloaded dra14003.dat\n",
      "Downloaded dra14004.dat\n",
      "Downloaded dra14005.dat\n",
      "Downloaded dra14006.dat\n",
      "Downloaded dra14007.dat\n",
      "Downloaded dra14008.dat\n",
      "Downloaded dra14009.dat\n",
      "Downloaded dra14010.dat\n",
      "Downloaded dra14011.dat\n",
      "Downloaded dra14012.dat\n",
      "Downloaded dra14013.dat\n",
      "Downloaded dra14014.dat\n",
      "Downloaded dra14015.dat\n",
      "Downloaded dra14016.dat\n",
      "Downloaded dra14017.dat\n",
      "Downloaded dra14018.dat\n",
      "Downloaded dra14019.dat\n",
      "Downloaded dra14020.dat\n",
      "Downloaded dra14021.dat\n",
      "Downloaded dra14022.dat\n",
      "Downloaded dra14023.dat\n",
      "Downloaded dra14024.dat\n",
      "Downloaded dra14025.dat\n",
      "Downloaded dra14026.dat\n",
      "Downloaded dra14027.dat\n",
      "Downloaded dra14028.dat\n",
      "Downloaded dra14029.dat\n",
      "Downloaded dra14030.dat\n",
      "Downloaded dra14031.dat\n",
      "Downloaded dra14032.dat\n",
      "Downloaded dra14033.dat\n",
      "Downloaded dra14034.dat\n",
      "Downloaded dra14035.dat\n",
      "Downloaded dra14036.dat\n",
      "Downloaded dra14037.dat\n",
      "Downloaded dra14038.dat\n",
      "Downloaded dra14039.dat\n",
      "Downloaded dra14040.dat\n",
      "Downloaded dra14041.dat\n",
      "Downloaded dra14042.dat\n",
      "Downloaded dra14043.dat\n",
      "Downloaded dra14044.dat\n",
      "Downloaded dra14045.dat\n",
      "Downloaded dra14046.dat\n",
      "Downloaded dra14047.dat\n",
      "Downloaded dra14048.dat\n",
      "Downloaded dra14049.dat\n",
      "Downloaded dra14050.dat\n",
      "Downloaded dra14051.dat\n",
      "Downloaded dra14052.dat\n",
      "Downloaded dra14053.dat\n",
      "Downloaded dra14054.dat\n",
      "Downloaded dra14055.dat\n",
      "Downloaded dra14056.dat\n",
      "Downloaded dra14057.dat\n",
      "Downloaded dra14058.dat\n",
      "Downloaded dra14059.dat\n",
      "Downloaded dra14060.dat\n",
      "Downloaded dra14061.dat\n",
      "Downloaded dra14062.dat\n",
      "Downloaded dra14063.dat\n",
      "Downloaded dra14064.dat\n",
      "Downloaded dra14065.dat\n",
      "Downloaded dra14066.dat\n",
      "Downloaded dra14067.dat\n",
      "Downloaded dra14068.dat\n",
      "Downloaded dra14069.dat\n",
      "Downloaded dra14070.dat\n",
      "Downloaded dra14071.dat\n",
      "Downloaded dra14072.dat\n",
      "Downloaded dra14073.dat\n",
      "Downloaded dra14074.dat\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m         current_date \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m365\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43mdownload_surfrad_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdra\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2014-01-01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2015-12-31\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 36\u001b[0m, in \u001b[0;36mdownload_surfrad_data\u001b[1;34m(station, start_date, end_date)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Download file\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 36\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurfrad_data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\Manuel\\miniconda3\\envs\\mlph\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Manuel\\miniconda3\\envs\\mlph\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Manuel\\miniconda3\\envs\\mlph\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\Manuel\\miniconda3\\envs\\mlph\\lib\\site-packages\\requests\\sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 746\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\Manuel\\miniconda3\\envs\\mlph\\lib\\site-packages\\requests\\models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Manuel\\miniconda3\\envs\\mlph\\lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\Manuel\\miniconda3\\envs\\mlph\\lib\\site-packages\\urllib3\\response.py:1060\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1060\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1062\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1063\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Manuel\\miniconda3\\envs\\mlph\\lib\\site-packages\\urllib3\\response.py:949\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 949\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    951\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Manuel\\miniconda3\\envs\\mlph\\lib\\site-packages\\urllib3\\response.py:873\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    870\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 873\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    875\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    876\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Manuel\\miniconda3\\envs\\mlph\\lib\\site-packages\\urllib3\\response.py:856\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\Manuel\\miniconda3\\envs\\mlph\\lib\\http\\client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 463\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Manuel\\miniconda3\\envs\\mlph\\lib\\http\\client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    502\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\Manuel\\miniconda3\\envs\\mlph\\lib\\socket.py:716\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    715\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 716\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Manuel\\miniconda3\\envs\\mlph\\lib\\ssl.py:1275\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1272\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1274\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\Manuel\\miniconda3\\envs\\mlph\\lib\\ssl.py:1133\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import ftplib\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def download_surfrad_data(station='dra', start_date='2014-01-01', end_date='2015-12-31'):\n",
    "    \"\"\"\n",
    "    Download SURFRAD data for a specific station and date range\n",
    "    station: 'dra' for Desert Rock\n",
    "    \"\"\"\n",
    "    # Base URL for SURFRAD data\n",
    "    base_url = \"https://gml.noaa.gov/aftp/data/radiation/surfrad/\"\n",
    "    \n",
    "    # Parse dates\n",
    "    start = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    \n",
    "    # Create directory for data\n",
    "    os.makedirs(f'surfrad_data/{station}', exist_ok=True)\n",
    "    \n",
    "    # Loop through all dates\n",
    "    current_date = start\n",
    "    while current_date <= end:\n",
    "        year = current_date.year\n",
    "        month = current_date.month\n",
    "        day = current_date.day\n",
    "\n",
    "        for day in range(1, 365):\n",
    "            # Construct URL for specific date\n",
    "            year_formatted = str(year)[2:]\n",
    "            file_name = f\"{station}{year_formatted}{day:03d}.dat\"\n",
    "            url = f\"{base_url}{station}/{year}/{file_name}\"\n",
    "            \n",
    "            # Download file\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    with open(f'surfrad_data/{station}/{file_name}', 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "                    print(f\"Downloaded {file_name}\")\n",
    "                else:\n",
    "                    print(f\"Failed to download {file_name}: {response.status_code}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {file_name}: {e}\")\n",
    "        \n",
    "        # Move to next day\n",
    "        current_date += timedelta(days=365)\n",
    "\n",
    "# Example usage\n",
    "download_surfrad_data(station='dra', start_date='2014-01-01', end_date='2015-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0     1    2     3     4     5          6           7           8   \\\n",
      "18    2014.0   1.0  1.0   1.0   4.0  37.0   4.616667  136.880000   -4.160000   \n",
      "19    2014.0   1.0  1.0   1.0   4.0  52.0   4.866667  139.868000   -4.060000   \n",
      "20    2014.0   1.0  1.0   1.0   5.0   7.0   5.116667  142.842667   -3.853333   \n",
      "21    2014.0   1.0  1.0   1.0   5.0  22.0   5.366667  145.790000   -3.620000   \n",
      "22    2014.0   1.0  1.0   1.0   5.0  37.0   5.616667  148.696000   -3.653333   \n",
      "...      ...   ...  ...   ...   ...   ...        ...         ...         ...   \n",
      "7082  2014.0  74.0  3.0  15.0  18.0  37.0  18.616667   42.464667  824.766667   \n",
      "7083  2014.0  74.0  3.0  15.0  18.0  52.0  18.866667   41.128000  846.186667   \n",
      "7084  2014.0  74.0  3.0  15.0  19.0   7.0  19.116667   40.044000  861.780000   \n",
      "7085  2014.0  74.0  3.0  15.0  19.0  22.0  19.366667   39.246000  874.673333   \n",
      "7086  2014.0  74.0  3.0  15.0  19.0  37.0  19.616667   38.746000  883.860000   \n",
      "\n",
      "              10          12          14          16          18          20  \\\n",
      "18     -0.866667   -0.240000    0.000000  253.973333  279.873333  279.593333   \n",
      "19     -0.420000    0.493333   -0.013333  253.493333  279.726667  279.453333   \n",
      "20     -0.506667    1.106667   -0.073333  252.440000  279.433333  279.106667   \n",
      "21     -0.626667    0.853333   -0.133333  250.446667  278.626667  278.293333   \n",
      "22     -0.566667    0.673333   -0.106667  250.033333  278.040000  277.773333   \n",
      "...          ...         ...         ...         ...         ...         ...   \n",
      "7082  168.953333  960.813333  111.740000  280.493333  294.880000  294.506667   \n",
      "7083  173.313333  968.420000  110.806667  282.360000  295.173333  294.813333   \n",
      "7084  176.413333  970.920000  112.040000  283.540000  295.486667  295.126667   \n",
      "7085  178.866667  972.920000  113.726667  284.186667  295.813333  295.466667   \n",
      "7086  181.326667  973.393333  116.760000  283.920000  296.346667  296.033333   \n",
      "\n",
      "              22          24          26          28          30          32  \\\n",
      "18    323.773333  279.386667  279.420000    0.000000    0.013333    0.000000   \n",
      "19    322.393333  279.393333  279.393333    0.000000    0.000000    0.000000   \n",
      "20    320.126667  279.026667  279.006667    0.000000    0.000000    0.000000   \n",
      "21    317.680000  278.493333  278.460000    0.000000    0.000000    0.000000   \n",
      "22    316.680000  277.820000  277.820000    0.000000    0.000000    0.000000   \n",
      "...          ...         ...         ...         ...         ...         ...   \n",
      "7082  473.840000  292.180000  292.540000  142.333333  355.160000  651.573333   \n",
      "7083  477.620000  292.493333  292.833333  150.900000  363.540000  666.986667   \n",
      "7084  482.560000  292.753333  293.120000  157.293333  369.620000  678.853333   \n",
      "7085  489.160000  293.253333  293.653333  162.160000  374.040000  688.306667   \n",
      "7086  491.846667  293.800000  294.160000  165.073333  375.980000  694.613333   \n",
      "\n",
      "              34          36         38         40        42          44  \\\n",
      "18    -69.793333  -69.793333   8.493333  23.166667  0.833333   56.353333   \n",
      "19    -68.886667  -68.886667   8.073333  24.000000  0.280000  147.726667   \n",
      "20    -67.706667  -67.706667   6.940000  25.846667  0.340000  313.520000   \n",
      "21    -67.213333  -67.213333   5.486667  28.893333  0.306667  279.853333   \n",
      "22    -66.653333  -66.653333   4.780000  30.053333  0.340000  315.906667   \n",
      "...          ...         ...        ...        ...       ...         ...   \n",
      "7082 -193.326667  458.240000  17.686667  17.293333  6.066667   34.880000   \n",
      "7083 -195.246667  471.746667  17.993333  17.806667  6.860000   27.840000   \n",
      "7084 -199.013333  479.840000  18.140000  17.413333  6.780000   37.666667   \n",
      "7085 -204.960000  483.333333  18.813333  15.866667  6.413333   41.166667   \n",
      "7086 -207.926667  486.700000  19.386667  13.193333  7.080000   23.680000   \n",
      "\n",
      "              46  \n",
      "18    908.693333  \n",
      "19    908.840000  \n",
      "20    908.813333  \n",
      "21    908.826667  \n",
      "22    908.900000  \n",
      "...          ...  \n",
      "7082  913.453333  \n",
      "7083  913.333333  \n",
      "7084  913.166667  \n",
      "7085  912.846667  \n",
      "7086  912.593333  \n",
      "\n",
      "[4514 rows x 28 columns]\n",
      "          0     1    2     3     4     5          6           7           8   \\\n",
      "0     2014.0   1.0  1.0   1.0   4.0  37.0   4.616667  136.880000   -4.160000   \n",
      "1     2014.0   1.0  1.0   1.0   4.0  52.0   4.866667  139.868000   -4.060000   \n",
      "2     2014.0   1.0  1.0   1.0   5.0   7.0   5.116667  142.842667   -3.853333   \n",
      "3     2014.0   1.0  1.0   1.0   5.0  22.0   5.366667  145.790000   -3.620000   \n",
      "4     2014.0   1.0  1.0   1.0   5.0  37.0   5.616667  148.696000   -3.653333   \n",
      "...      ...   ...  ...   ...   ...   ...        ...         ...         ...   \n",
      "2679  2014.0  44.0  2.0  13.0  18.0  37.0  18.616667   53.401333  618.646667   \n",
      "2680  2014.0  44.0  2.0  13.0  18.0  52.0  18.866667   52.228667  640.193333   \n",
      "2681  2014.0  44.0  2.0  13.0  19.0   7.0  19.116667   51.273333  697.226667   \n",
      "2682  2014.0  44.0  2.0  13.0  19.0  22.0  19.366667   50.547333  704.533333   \n",
      "2683  2014.0  44.0  2.0  13.0  19.0  37.0  19.616667   50.059333  711.493333   \n",
      "\n",
      "              10          12          14          16          18          20  \\\n",
      "0      -0.866667   -0.240000    0.000000  253.973333  279.873333  279.593333   \n",
      "1      -0.420000    0.493333   -0.013333  253.493333  279.726667  279.453333   \n",
      "2      -0.506667    1.106667   -0.073333  252.440000  279.433333  279.106667   \n",
      "3      -0.626667    0.853333   -0.133333  250.446667  278.626667  278.293333   \n",
      "4      -0.566667    0.673333   -0.106667  250.033333  278.040000  277.773333   \n",
      "...          ...         ...         ...         ...         ...         ...   \n",
      "2679  125.680000  734.913333  181.173333  290.266667  294.280000  294.206667   \n",
      "2680  129.806667  779.726667  164.966667  291.220000  295.166667  294.980000   \n",
      "2681  142.320000  963.233333   95.826667  291.853333  295.980000  295.646667   \n",
      "2682  144.193333  972.733333   85.820000  293.460000  296.600000  296.240000   \n",
      "2683  145.613333  991.093333   75.013333  294.453333  297.086667  296.686667   \n",
      "\n",
      "              22          24          26          28          30          32  \\\n",
      "0     323.773333  279.386667  279.420000    0.000000    0.013333    0.000000   \n",
      "1     322.393333  279.393333  279.393333    0.000000    0.000000    0.000000   \n",
      "2     320.126667  279.026667  279.006667    0.000000    0.000000    0.000000   \n",
      "3     317.680000  278.493333  278.460000    0.000000    0.000000    0.000000   \n",
      "4     316.680000  277.820000  277.820000    0.000000    0.000000    0.000000   \n",
      "...          ...         ...         ...         ...         ...         ...   \n",
      "2679  455.733333  291.940000  292.293333   83.813333  274.380000  493.406667   \n",
      "2680  463.206667  292.980000  293.326667   89.340000  284.246667  513.066667   \n",
      "2681  473.213333  293.986667  294.340000   98.180000  307.540000  556.173333   \n",
      "2682  479.093333  294.780000  295.106667  102.133333  310.173333  559.720000   \n",
      "2683  484.706667  295.326667  295.646667  104.713333  313.093333  565.726667   \n",
      "\n",
      "              34          36         38         40        42          44  \\\n",
      "0     -69.793333  -69.793333   8.493333  23.166667  0.833333   56.353333   \n",
      "1     -68.886667  -68.886667   8.073333  24.000000  0.280000  147.726667   \n",
      "2     -67.706667  -67.706667   6.940000  25.846667  0.340000  313.520000   \n",
      "3     -67.213333  -67.213333   5.486667  28.893333  0.306667  279.853333   \n",
      "4     -66.653333  -66.653333   4.780000  30.053333  0.340000  315.906667   \n",
      "...          ...         ...        ...        ...       ...         ...   \n",
      "2679 -165.453333  327.940000  17.860000  18.886667  1.080000  289.113333   \n",
      "2680 -171.980000  341.073333  18.693333  17.966667  1.100000  215.906667   \n",
      "2681 -181.353333  374.813333  19.266667  17.186667  1.320000  244.520000   \n",
      "2682 -185.640000  374.073333  19.400000  17.086667  1.600000  247.880000   \n",
      "2683 -190.253333  375.480000  19.840000  16.586667  1.220000  261.280000   \n",
      "\n",
      "              46  \n",
      "0     908.693333  \n",
      "1     908.840000  \n",
      "2     908.813333  \n",
      "3     908.826667  \n",
      "4     908.900000  \n",
      "...          ...  \n",
      "2679  907.100000  \n",
      "2680  906.680000  \n",
      "2681  906.233333  \n",
      "2682  905.906667  \n",
      "2683  905.500000  \n",
      "\n",
      "[2684 rows x 28 columns]\n",
      "[253.97333333 253.49333333 252.44       ... 291.85333333 293.46\n",
      " 294.45333333]\n",
      "[[253.97333333 253.49333333 252.44       ... 277.59333333 278.6\n",
      "  280.17333333]\n",
      " [268.52       267.86       265.88       ... 272.66666667 273.94\n",
      "  274.62      ]\n",
      " [256.4        256.86666667 256.65333333 ... 267.16       269.18\n",
      "  270.28666667]\n",
      " ...\n",
      " [267.86666667 266.9        267.39333333 ... 274.43333333 274.10666667\n",
      "  274.2       ]\n",
      " [266.27333333 266.22       266.70666667 ... 291.8        294.1\n",
      "  297.86      ]\n",
      " [295.14666667 291.53333333 289.42666667 ... 291.85333333 293.46\n",
      "  294.45333333]]\n",
      "Training data shape: (44, 61)\n",
      "Testing data shape: (30, 61)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def read_surfrad_data(data_dir):\n",
    "    \"\"\"\n",
    "    Read all SURFRAD .dat files in the specified directory\n",
    "    Returns a DataFrame with parsed data\n",
    "    \"\"\"\n",
    "    # Get all .dat files in the directory\n",
    "    file_pattern = os.path.join(data_dir, '*.dat')\n",
    "    data_files = glob.glob(file_pattern)\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for file_path in data_files:\n",
    "        try:\n",
    "            # SURFRAD files have a specific format with headers\n",
    "            # Skip the header lines (first 2 rows)\n",
    "            df = pd.read_csv(file_path, sep='\\s+', skiprows=2, header=None)\n",
    "            \n",
    "            # Extract filename for date information\n",
    "            \"\"\" filename = os.path.basename(file_path)\n",
    "            date_str = filename[3:11]  # Extract YYYYMMDD from draYYYYMMDD.dat\n",
    "            file_date = datetime.strptime(date_str, '%Y%m%d')\n",
    "            print(df[1]) \"\"\"\n",
    "        \n",
    "            \n",
    "            # Create timestamp from date and time columns\n",
    "            \"\"\" \n",
    "            # Filter out bad values (often marked as -9999.9)\n",
    "            parsed_df = parsed_df[parsed_df['irradiance'] > -999]\n",
    "            \n",
    "            all_data.append(parsed_df) \"\"\"\n",
    "            cols_to_drop = df.columns[9::2]  # Every second column from index 9 onward\n",
    "            df_cleaned = df.drop(columns=cols_to_drop)\n",
    "            all_data.append(df_cleaned)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    # Combine all data files\n",
    "    if not all_data:\n",
    "        raise ValueError(\"No valid data files found\")\n",
    "    \n",
    "    combined_data = pd.concat(all_data)\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    #combined_data = combined_data.sort_values('timestamp')\n",
    "    \n",
    "    # Set timestamp as index\n",
    "    #combined_data.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    return combined_data\n",
    "\n",
    "# Load your NOAA SURFRAD data\n",
    "data_dir = 'final_project\\\\surfrad_data\\\\dra'  \n",
    "data_dir = os.path.abspath('surfrad_data/dra')\n",
    "df = read_surfrad_data(data_dir)\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "\n",
    "# Convert to 15-minute resolution by averaging\n",
    "def convert_to_15min_resolution(df):\n",
    "    # Ensure the DataFrame index is a range index\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Group the DataFrame by every 15 rows and calculate the mean for each group\n",
    "    df_15min = df.groupby(df.index // 15).mean()\n",
    "    \n",
    "    return df_15min\n",
    "\n",
    "# Now we can continue with the rest of your pipeline\n",
    "df_15min = convert_to_15min_resolution(df)\n",
    "\n",
    "# Extract only points 18-78 for each day (corresponding to daylight hours)\n",
    "def extract_daylight_hours(df):\n",
    "    # Define chunk size and slice range\n",
    "    chunk_size = 96\n",
    "    start, end = 18, 79  # Keep indices 18 to 78 (end index is exclusive)\n",
    "\n",
    "    # Process data in chunks\n",
    "    filtered_data = pd.concat([df.iloc[i + start : i + end] for i in range(0, len(df), chunk_size)])\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "df_daylight = extract_daylight_hours(df_15min)\n",
    "\n",
    "# Split into training and testing sets\n",
    "def split_train_test(df):\n",
    "    data_points_per_day = 61\n",
    "    total_days = len(df) // data_points_per_day\n",
    "\n",
    "    train_days = int(total_days * 0.6)  # 70% for training\n",
    "    test_days = total_days - train_days\n",
    "\n",
    "    train_index = train_days * data_points_per_day\n",
    "    test_index = train_index + (test_days * data_points_per_day)\n",
    "\n",
    "    df_train = df.iloc[:train_index].reset_index(drop=True)\n",
    "    df_test = df.iloc[train_index:test_index].reset_index(drop=True)\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "print(df_daylight)\n",
    "train_data, test_data = split_train_test(df_daylight)\n",
    "print(train_data)\n",
    "\n",
    "# Extract the irradiance values as numpy arrays for model training\n",
    "print(train_data[16].values)\n",
    "X_train = train_data[16].values.reshape(-1, 61)  # 61 points per day (18-78)\n",
    "X_test = test_data[16].values.reshape(-1, 61) # look up what the right index is for the dataset \n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "print(X_train)\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training data shape: {X_train_normalized.shape}\")\n",
    "print(f\"Testing data shape: {X_test_normalized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modify the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Update the model architecture based on paper's specifications\n",
    "def update_model_architecture(model):\n",
    "    # Generator and discriminator with 2 hidden layers of 124 neurons each\n",
    "    generator = models.Sequential([\n",
    "        layers.Dense(124, activation='relu', input_shape=(60,)),  # 60-dim uniform noise\n",
    "        layers.Dense(124, activation='relu'),\n",
    "        layers.Dense(61)  # Output layer (61 points for positions 18-78)\n",
    "    ])\n",
    "    \n",
    "    discriminator = models.Sequential([\n",
    "        layers.Dense(124, activation='relu', input_shape=(61,)),\n",
    "        layers.Dense(124, activation='relu'),\n",
    "        layers.Dense(10)  # 10 classes output\n",
    "    ])\n",
    "    \n",
    "    # Update model properties\n",
    "    model.generator = generator\n",
    "    model.discriminator = discriminator\n",
    "    model.latent_dim = 60\n",
    "    model.generator_optimizer = tf.keras.optimizers.Adam()\n",
    "    model.discriminator_optimizer = tf.keras.optimizers.Adam()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Update your existing model\n",
    "wgan_model = WeatherClassificationWGANGP()\n",
    "wgan_model = update_model_architecture(wgan_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modify the Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_train(model, dataset, epochs, batch_size=64):\n",
    "    \"\"\"Modified training procedure to match paper specifications\"\"\"\n",
    "    steps_per_epoch = len(dataset) // batch_size\n",
    "    \n",
    "    disc_losses = []\n",
    "    gen_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_disc_losses = []\n",
    "        epoch_gen_losses = []\n",
    "        \n",
    "        for step in range(steps_per_epoch):\n",
    "            # Get real images batch\n",
    "            indices = np.random.randint(0, len(dataset), batch_size)\n",
    "            real_data = dataset[indices]\n",
    "            # Get weather class labels based on your classification scheme\n",
    "            labels = np.zeros(batch_size)  # Replace with actual labels\n",
    "            \n",
    "            # Train discriminator for 15 steps (as per paper)\n",
    "            for _ in range(15):\n",
    "                d_loss, real_loss, fake_loss, gp = model.train_discriminator(real_data, labels)\n",
    "                epoch_disc_losses.append(d_loss.numpy())\n",
    "            \n",
    "            # Train generator for 1 step\n",
    "            g_loss = model.train_generator()\n",
    "            epoch_gen_losses.append(g_loss.numpy())\n",
    "        \n",
    "        # Store and print losses\n",
    "        disc_losses.append(np.mean(epoch_disc_losses))\n",
    "        gen_losses.append(np.mean(epoch_gen_losses))\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"D loss: {disc_losses[-1]:.4f}, G loss: {gen_losses[-1]:.4f}\")\n",
    "    \n",
    "    return disc_losses, gen_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Implement the Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn1d_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=(61, 1)),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(64, kernel_size=5, activation='relu'),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Conv1D(64, kernel_size=8, activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')  # 10 weather classes\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_cnn2d_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(64, kernel_size=(1, 1), activation='relu', input_shape=(61, 1, 1)),\n",
    "        layers.Conv2D(64, kernel_size=(2, 1), activation='relu'),\n",
    "        layers.Conv2D(64, kernel_size=(3, 2), activation='relu'),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')  # 10 weather classes\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_mlp_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Flatten(input_shape=(61,)),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')  # 10 weather classes\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train SVM and KNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def train_svm_model(X_train, y_train):\n",
    "    # Grid search for optimal parameters\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': [0.001, 0.01, 0.1, 1],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "    \n",
    "    svm = SVC()\n",
    "    grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best SVM parameters: {grid_search.best_params_}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def train_knn_model(X_train, y_train):\n",
    "    # Grid search for optimal parameters\n",
    "    param_grid = {\n",
    "        'n_neighbors': [3, 5, 7, 9, 11],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'metric': ['euclidean', 'manhattan']\n",
    "    }\n",
    "    \n",
    "    knn = KNeighborsClassifier()\n",
    "    grid_search = GridSearchCV(knn, param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best KNN parameters: {grid_search.best_params_}\")\n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Complete Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_full_pipeline(train_data, test_data, epochs=100):\n",
    "    # Step 1: Train WGAN-GP model\n",
    "    wgan_model = WeatherClassificationWGANGP()\n",
    "    wgan_model = update_model_architecture(wgan_model)\n",
    "    disc_losses, gen_losses = modified_train(wgan_model, train_data, epochs)\n",
    "    \n",
    "    # Step 2: Generate synthetic data\n",
    "    n_synthetic = 10000  # Define how many synthetic samples to generate\n",
    "    synthetic_data, synthetic_labels = wgan_model.generate_synthetic_data(n_synthetic)\n",
    "    \n",
    "    # Denormalize synthetic data\n",
    "    synthetic_data_denorm = scaler.inverse_transform(synthetic_data)\n",
    "    \n",
    "    # Step 3: Combine real and synthetic data\n",
    "    X_combined = np.vstack([train_data, synthetic_data])\n",
    "    y_combined = np.concatenate([train_labels, synthetic_labels])  # Assuming you have train_labels\n",
    "    \n",
    "    # Step 4: Train classification models\n",
    "    # CNN1D\n",
    "    cnn1d = build_cnn1d_model()\n",
    "    X_cnn1d = X_combined.reshape(-1, 61, 1)  # Reshape for 1D convolution\n",
    "    cnn1d.fit(X_cnn1d, y_combined, epochs=50, batch_size=32, validation_split=0.2)\n",
    "    \n",
    "    # CNN2D\n",
    "    cnn2d = build_cnn2d_model()\n",
    "    X_cnn2d = X_combined.reshape(-1, 61, 1, 1)  # Reshape for 2D convolution\n",
    "    cnn2d.fit(X_cnn2d, y_combined, epochs=50, batch_size=32, validation_split=0.2)\n",
    "    \n",
    "    # MLP\n",
    "    mlp = build_mlp_model()\n",
    "    mlp.fit(X_combined, y_combined, epochs=50, batch_size=32, validation_split=0.2)\n",
    "    \n",
    "    # SVM\n",
    "    svm = train_svm_model(X_combined, y_combined)\n",
    "    \n",
    "    # KNN\n",
    "    knn = train_knn_model(X_combined, y_combined)\n",
    "    \n",
    "    # Step 5: Evaluate models on test data\n",
    "    X_test_cnn1d = test_data.reshape(-1, 61, 1)\n",
    "    X_test_cnn2d = test_data.reshape(-1, 61, 1, 1)\n",
    "    \n",
    "    print(\"CNN1D Accuracy:\", cnn1d.evaluate(X_test_cnn1d, test_labels)[1])\n",
    "    print(\"CNN2D Accuracy:\", cnn2d.evaluate(X_test_cnn2d, test_labels)[1])\n",
    "    print(\"MLP Accuracy:\", mlp.evaluate(test_data, test_labels)[1])\n",
    "    print(\"SVM Accuracy:\", svm.score(test_data, test_labels))\n",
    "    print(\"KNN Accuracy:\", knn.score(test_data, test_labels))\n",
    "    \n",
    "    return {\n",
    "        'wgan_model': wgan_model,\n",
    "        'cnn1d': cnn1d,\n",
    "        'cnn2d': cnn2d,\n",
    "        'mlp': mlp,\n",
    "        'svm': svm,\n",
    "        'knn': knn\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Classifier:\n",
    "    \"\"\"Convolutional Neural Network for Weather Classification\"\"\"\n",
    "    def __init__(self, img_shape=(64, 64, 3), num_classes=10):\n",
    "        self.img_shape = img_shape\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Build model\n",
    "        self.model = self.build_model()\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Build CNN model for weather classification\"\"\"\n",
    "        model = models.Sequential([\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', input_shape=self.img_shape),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(self.num_classes)\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    def train(self, train_images, train_labels, validation_data=None, epochs=10, batch_size=32):\n",
    "        \"\"\"Train the CNN classifier\"\"\"\n",
    "        # Set up callbacks\n",
    "        checkpoint = ModelCheckpoint(\n",
    "            'model_checkpoints/cnn_classifier_best.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            train_images, train_labels,\n",
    "            validation_data=validation_data,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[checkpoint, early_stopping]\n",
    "        )\n",
    "        \n",
    "        # Plot training history\n",
    "        self.plot_training_history(history)\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def plot_training_history(self, history):\n",
    "        \"\"\"Plot and save training history\"\"\"\n",
    "        # Plot accuracy\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.title('CNN Classifier - Accuracy')\n",
    "        \n",
    "        # Plot loss\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('CNN Classifier - Loss')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('results/cnn_classifier_training_history.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def evaluate(self, test_images, test_labels):\n",
    "        \"\"\"Evaluate the CNN classifier\"\"\"\n",
    "        return self.model.evaluate(test_images, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
